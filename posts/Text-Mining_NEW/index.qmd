---
title: "Text Mining"
author: "Sabrina Michl"
date: "2023-01-31"
categories: [code, analysis]
bibliography: ref.bib
image: "image.jpg"
---

# 1. Preliminary Note

For this analysis we use the dataset from @data/0B5VML_2019 out of the zip archive @data/0B5VML/XIUWJ7_2019. The data are licensed according to Attribution 4.0 International (CC-BY-4.0). The used word embeddings are from @grave2018learning. The data are licensed according to Attribution-ShareAlike 3.0 Unported (CC-BY-SA 3.0).

The picture to this post from the welcome page, is from <a href="https://pixabay.com/de/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=7691355">Gerd Altmann</a> at <a href="https://pixabay.com/de//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=7691355">Pixabay</a>

# 2. Load The Packages

```{r output=FALSE}
library(tidyverse)
library(rio)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(lsa)
library(discrim)
library(naivebayes)
library(tictoc)
library(fastrtext)
library(remoji)
library(tokenizers)
```

# 3. Load Dataset And Minor Changes

## 3.1 Train Dataset

```{r output=FALSE}
d_train <- read_tsv("C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.training.txt", col_names = FALSE)
```

### Rename Columns

```{r}
names(d_train) <- c("text", "c1", "c2")
```

### Add ID Column

```{r}
d_train <- d_train %>%
mutate(id = row_number()) %>%
select(id, everything())
```

## 3.2 Test Dataset

```{r output=FALSE}
d_test <- read_tsv("C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.test.txt", col_names = FALSE)
```

### Rename Columns

```{r}
names(d_test) <- c("text", "c1", "c2")
```

### Add ID Column

```{r}
d_test <- d_test %>%
mutate(id = row_number()) %>%
select(id, everything())
```

# 4. Explore Dataset

```{r}
train_toc <- d_train %>%
unnest_tokens(output = token, input = text)
train_toc
```

> First we tokenize the dataset d_train.

## 4.1 Insert `Stopwords_de`

```{r}
data(stopwords_de, package = "lsa")
stopwords_de <- tibble(word = stopwords_de)
stopwords_de <- stopwords_de %>%
rename(token = word)
```

> After that we use the stopwords_de to `anti_join` this with train_toc dataset.

```{r}
train_toc2 <- train_toc %>%
anti_join(stopwords_de)
```

## Show The Important Words

```{r}
train_toc2 <- train_toc2 %>%
count(token, sort = TRUE)
```

### Plot

```{r}
train_toc2 %>%
slice_head(n=20) %>%
ggplot()+
aes(y=reorder(factor(token), n), x = n, color = token)+
geom_col(aes(fill = token, alpha = 2.5)) +
ggtitle("The most used words") +
ylab("token")+
xlab("quantity")+
theme_minimal()+
theme(legend.position = "none")
```

> We see that to most used word is "lbr". We could inspect the dataset way deeper, e.g. do a manual sentimentanalysis, do a lemmatization or stem the words. But we will have a look at these processes in the different machine learning algorithms following now.

# 5. Preparation

## 5.1 Define Recipe - rec1 - TF-IDF

```{r}
rec1 <-
recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>%
update_role(id, new_role = "id") %>%
step_tokenize(text) %>%
step_stopwords(text, language = "de", stopword_source = "snowball") %>%
step_stem(text) %>%
step_tfidf(text) %>%
step_normalize(all_numeric_predictors())
rec1
```

### Prep & Bake - rec1

```{r}
rec1_prep <- rec1 %>%
prep() %>%
recipes::bake(new_data = NULL)
```

## 5.2 Define Recipe - rec2 - word embedding

After fitting all the models with the training-resample, I have decided to not fit the easy word embedding recipe (rec2), because of the long computing time! We have a large amount of data and extensive recipes, so when I raise the resamples for the prediction, R Studio quits the session. And because of that I only do a prediction-resample of 3 folds and 2 repetitions.

-   [training:]{.underline} v = 2, repeats = 1
-   [prediction:]{.underline} v = 3, repeats = 2

```{r}
#rec2 <-
#recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>%
#update_role(id, new_role = "id") %>%
#step_tokenize(text) %>%
#step_stopwords(text, language = "de", stopword_source = "snowball") %>%
#step_word_embeddings(text, embeddings = word_embedding_text)
```

### Insert The Predefined List

```{r}
out_file_model <- "C:/Users/sapi-/OneDrive - Hochschule fÃ¼r Angewandte Wissenschaften Ansbach/Desktop/AWM/angewandte Wirtschats- und Medienpsychologie/5. Semester/Word_Embedding/de.300.bin"
```

```{r}
file.exists(out_file_model)
```

```{r}
fasttext_model <- load_model(out_file_model)
dictionary <- get_dictionary(fasttext_model)
get_word_vectors(fasttext_model, c("menschen")) %>% `[`(1:10)
```

```{r}
print(head(dictionary, 10))
```

```{r}
word_embedding_text <- tibble(word = dictionary)
```

```{r}
options(mc.cores = parallel::detectCores())
words_vecs <- get_word_vectors(fasttext_model)
```

```{r output=FALSE}
word_embedding_text <-
word_embedding_text %>%
bind_cols(words_vecs)
```

```{r output=FALSE}
names(word_embedding_text) <- c("word", paste0("v", sprintf("%03d", 1:301)))
```

## 5.3 Define Recipe - rec3 - Word Embeddings

### Insert The Helperfunctions

We are using the package \[pradadata\] (https://github.com/sebastiansauer/pradadata) by @sebastian_sauer_2018_1996614. The data are licensed according to General Public License 3 (GLP-3).

```{r}
data("schimpwoerter", package = "pradadata")
data("sentiws", package = "pradadata")
data("wild_emojis", package = "pradadata")
source("C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/helper/helper_funs.R")
```

### rec3

```{r}
rec3 <-
recipe(c1 ~., data = select(d_train, text, c1, id)) %>%
update_role(id, new_role = "id") %>%
step_text_normalization(text) %>%
step_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>%
step_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>%
step_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>%
step_mutate(text_copy = text) %>%
step_textfeature(text_copy) %>%
step_tokenize(text) %>%
step_stopwords(text, language = "de", stopword_source = "snowball") %>%
step_stem(text) %>%
step_word_embeddings(text, embeddings = word_embedding_text)
```

```{r}
rec3_prep <- rec3 %>%
prep() %>%
recipes::bake(new_data = NULL)
```

## 5.4 Define Recipe - rec4 - TF-IDF

### rec4

```{r}
rec4 <-
recipe(c1 ~., data = select(d_train, text, c1, id)) %>%
update_role(id, new_role = "id") %>% 
step_text_normalization(text) %>%
step_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>%
step_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>%
step_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>%
step_mutate(text_copy = text) %>%
step_textfeature(text_copy) %>%
step_tokenize(text) %>%
step_stopwords(text, language = "de", stopword_source = "snowball") %>%
step_stem(text) %>%
step_tfidf(text)
```

```{r}
rec4_prep <- rec4 %>%
prep() %>%
recipes::bake(new_data = NULL)
```

# 6. Build Resamples

I decided to go with the v-fold-cross-validation as it is rather time-efficient. We have a large amount of data with extremely extensive recipes, so the run-time would be enormous, if we would try another resampling option, e.g. bootstrapping.

```{r}
folds <- vfold_cv(data = d_train,
v = 3,
repeats = 2,
strata = c1)
```

# 7. Build the Penalty-Grid

```{r}
lambda_grid <- grid_regular(penalty(), levels = 20)
```

# 8. Build the Models

## 8.1 Null Model

## 8.2 Lasso-L1 With TF-IDF

According to the large amount of data, I decided to not run the Null Model and the L1-TF-IDF with rec1.

## 8.3 Ridge-Regression-L2 With TF-IDF

### L2-Model

```{r}
l2_83_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%
set_engine("glmnet") %>%
set_mode("classification")
l2_83_mod
```

### Define The Workflow

```{r}
l2_83_wf <-workflow() %>%
add_recipe(rec1) %>%
add_model(l2_83_mod)
l2_83_wf
```

### Resampling & Model Quality

```{r}
options(mc.cores = parallel::detectCores())
l2_83_wf_fit <- tune_grid(
l2_83_wf,
folds,
grid = lambda_grid,
control = control_resamples(save_pred = TRUE)
)
```

### Model Performance

```{r}
l2_83_wf_fit_performance <- collect_metrics(l2_83_wf_fit)
l2_83_wf_fit_performance
```

```{r}
l2_83_wf_fit_preds <- collect_predictions(l2_83_wf_fit)
```

```{r}
l2_83_wf_fit_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```

> The model is slightly better compared to the previous one. Which is also reflected in the shape. It shows a tendency towards sensivity = 1.

### Select The Best

```{r}
chosen_auc_l2_83_wf_fit <-
l2_83_wf_fit %>%
select_by_one_std_err(metric = "roc_auc", -penalty)
chosen_auc_l2_83_wf_fit
```

```{r}
conf_mat_resampled(l2_83_wf_fit, tidy = FALSE, parameter = select_best(l2_83_wf_fit)) %>% 
  autoplot(type = "heatmap") 
```

## 8.4 Lasso-L1 With Word Embeddings

Like the null model, 8.1 and the L1 with TF-IDF, 8.2 I decided to kick them out of the analysis. Unfortunatly I had to the complexity was too high, so R could not take the amount of Data.

## 8.5 Ridge-Regression-L2 with TF-IDF

### L2-Model

```{r}
l2_85_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%
set_engine("glmnet") %>%
set_mode("classification")
l2_85_mod
```

### Define The Workflow

```{r}
l2_85_wf <-workflow() %>%
add_recipe(rec3) %>%
add_model(l2_85_mod)
l2_85_wf
```

### Resampling & Model Quality

```{r}
options(mc.cores = parallel::detectCores())
l2_85_wf_fit <- tune_grid(
l2_85_wf,
folds,
grid = lambda_grid,
control = control_resamples(save_pred = TRUE)
)
```

### Model Performance

```{r}
l2_85_wf_performance <- collect_metrics(l2_85_wf_fit)
l2_85_wf_performance
```

```{r}
l2_85_wf_fit_preds <- collect_predictions(l2_85_wf_fit)
```

```{r}
l2_85_wf_fit_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```

> This is similar to `l2_83_wf_fit.` But this one is more balanced in the center!

### Select The Best

```{r}
chosen_auc_l2_85_wf_fit <-
l2_85_wf_fit %>%
select_by_one_std_err(metric = "roc_auc", -penalty)
chosen_auc_l2_85_wf_fit
```

```{r}
conf_mat_resampled(l2_85_wf_fit, tidy = FALSE, parameter = select_best(l2_85_wf_fit)) %>% 
  autoplot(type = "heatmap") 
```

## 8.6 Lasso-L1 With TF-IDF

### L1-Model

```{r}
l1_86_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet") %>%
set_mode("classification")
l1_86_mod
```

### Define The Workflow

```{r}
l1_86_wf <-workflow() %>%
add_recipe(rec4) %>%
add_model(l1_86_mod)
l1_86_wf
```

### Resampling & Model Quality

```{r}
options(mc.cores = parallel::detectCores())
l1_86_wf_fit <- tune_grid(
l1_86_wf,
folds,
grid = lambda_grid,
control = control_resamples(save_pred = TRUE)
)
```

### Model Performance

```{r}
l1_86_wf_performance <- collect_metrics(l1_86_wf_fit)
l1_86_wf_performance
```

```{r}
l1_86_wf_fit_preds <- collect_predictions(l1_86_wf_fit)
```

```{r}
l1_86_wf_fit_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```

> Here you can see a good balanced model, which shows a good sensitivity and specificity!

```{r}
conf_mat_resampled(l1_86_wf_fit, tidy = FALSE, parameter = select_best(l1_86_wf_fit)) %>% 
  autoplot(type = "heatmap") 
```

### Select The Best

```{r}
chosen_auc_l1_86_wf_fit <-
l1_86_wf_fit %>%
select_by_one_std_err(metric = "roc_auc", -penalty)
chosen_auc_l1_86_wf_fit
```

## 8.7 Ridge-Regression-L2 With TF-IDF

### L2-Model

```{r}
l2_87_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%
set_engine("glmnet") %>%
set_mode("classification")
l2_87_mod
```

### Define The Workflow

```{r}
l2_87_wf <-workflow() %>%
add_recipe(rec4) %>%
add_model(l2_87_mod)
l2_87_wf
```

### Resampling & Model Quality

```{r}
options(mc.cores = parallel::detectCores())
l2_87_wf_fit <- tune_grid(
l2_87_wf,
folds,
grid = lambda_grid,
control = control_resamples(save_pred = TRUE)
)
```

### Model Performance

```{r}
l2_87_wf_performance <- collect_metrics(l2_87_wf_fit)
l2_87_wf_performance
```

```{r}
l2_87_wf_fit_preds <- collect_predictions(l2_87_wf_fit)
```

```{r}
l2_87_wf_fit_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```

Here you can see a good balanced model, which shows a good sensitivity and specificity, with a slight tendency towards sensitivity = 1.

### Select The Best

```{r}
chosen_auc_l2_87_wf_fit <-
l2_87_wf_fit %>%
select_by_one_std_err(metric = "roc_auc", -penalty)
chosen_auc_l2_87_wf_fit
```

```{r}
conf_mat_resampled(l2_87_wf_fit, tidy = FALSE, parameter = select_best(l2_87_wf_fit)) %>% 
  autoplot(type = "heatmap") 
```

# 9. Prediction
