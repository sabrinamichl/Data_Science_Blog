---
title: "Text Mining"
author: "Sabrina Michl"
date: "2023-01-31"
categories: [code, analysis]
bibliography: ref.bib
image: "image.jpg"
---


# 1. Preliminary Note

For this analysis we use the dataset from @data/0B5VML_2019 out of the zip archive @data/0B5VML/XIUWJ7_2019. The data are licensed according to Attribution 4.0 International (CC-BY-4.0). The used word embeddings are from @grave2018learning. The data are licensed according to Attribution-ShareAlike 3.0 Unported (CC-BY-SA 3.0).

The picture to this post from the welcome page, is from <a href="https://pixabay.com/de/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=7691355">Gerd Altmann</a> at <a href="https://pixabay.com/de//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=7691355">Pixabay</a>

# 2. Load The Packages


```{r output=FALSE}
library(tidyverse)
library(rio)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(lsa)
library(discrim)
library(naivebayes)
library(tictoc)
library(fastrtext)
library(remoji)
library(tokenizers)
```


# 3. Load Dataset And Minor Changes

## 3.1 Train Dataset


```{r output=FALSE}
d_train <- read_tsv("C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.training.txt", col_names = FALSE)
```


### Rename Columns


```{r}
names(d_train) <- c("text", "c1", "c2")
```


### Add ID Column


```{r}
d_train <- d_train %>%
mutate(id = row_number()) %>%
select(id, everything())
```


## 3.2 Test Dataset


```{r output=FALSE}
d_test <- read_tsv("C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.test.txt", col_names = FALSE)
```


### Rename Columns


```{r}
names(d_test) <- c("text", "c1", "c2")
```


### Add ID Column


```{r}
d_test <- d_test %>%
mutate(id = row_number()) %>%
select(id, everything())
```


# 4. Explore Dataset


```{r}
train_toc <- d_train %>%
unnest_tokens(output = token, input = text)
train_toc
```


> First tokenize the dataset d_train.

## 4.1 Insert `Stopwords_de`


```{r}
data(stopwords_de, package = "lsa")
stopwords_de <- tibble(word = stopwords_de)
stopwords_de <- stopwords_de %>%
rename(token = word)
```


> After that we use the stopwords_de to `anti_join` this with train_toc dataset.


```{r}
train_toc2 <- train_toc %>%
anti_join(stopwords_de)
```


## Show The Important Words


```{r}
train_toc2 <- train_toc2 %>%
count(token, sort = TRUE)
```


### Plot


```{r}
train_toc2 %>%
slice_head(n=20) %>%
ggplot()+
aes(y=reorder(factor(token), n), x = n, color = token)+
geom_col(aes(fill = token, alpha = 2.5)) +
ggtitle("The most used words") +
ylab("token")+
xlab("quantity")+
theme_minimal()+
theme(legend.position = "none")
```


> We see that to most used word is "lbr". We could inspect the dataset way deeper, e.g. do a manual sentimentanalysis, do a lemmatization or stem the words. But we will have a look at these processes in the different machine learning algorithms following now.

# 5. Preparation

## 5.1 Define Recipe - rec1 - TF-IDF


```{r}
rec1 <-
recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>%
update_role(id, new_role = "id") %>%
step_tokenize(text) %>%
step_stopwords(text, language = "de", stopword_source = "snowball") %>%
step_stem(text) %>%
step_tfidf(text) %>%
step_normalize(all_numeric_predictors())
rec1
```


### Prep & Bake - rec1


```{r}
rec1_prep <- rec1 %>%
prep() %>%
recipes::bake(new_data = NULL)
```


## 5.2 Define Recipe - rec2 - word embedding

After fitting all the models with the training-resample, I have decided to not fit the easy word embedding recipe (rec2), because of the long computing time!


```{r}
#rec2 <-
#recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>%
#update_role(id, new_role = "id") %>%
#step_tokenize(text) %>%
#step_stopwords(text, language = "de", stopword_source = "snowball") %>%
#step_word_embeddings(text, embeddings = word_embedding_text)
```


### Insert The Predefined List


```{r}
out_file_model <- "C:/Users/sapi-/OneDrive - Hochschule fÃ¼r Angewandte Wissenschaften Ansbach/Desktop/AWM/angewandte Wirtschats- und Medienpsychologie/5. Semester/Word_Embedding/de.300.bin"
```

```{r}
file.exists(out_file_model)
```

```{r}
fasttext_model <- load_model(out_file_model)
dictionary <- get_dictionary(fasttext_model)
get_word_vectors(fasttext_model, c("menschen")) %>% `[`(1:10)
```

```{r}
print(head(dictionary, 10))
```

```{r}
word_embedding_text <- tibble(word = dictionary)
```

```{r}
options(mc.cores = parallel::detectCores())
words_vecs <- get_word_vectors(fasttext_model)
```

```{r output=FALSE}
word_embedding_text <-
word_embedding_text %>%
bind_cols(words_vecs)
```

```{r output=FALSE}
names(word_embedding_text) <- c("word", paste0("v", sprintf("%03d", 1:301)))
```


## 5.3 Define Recipe - rec3 - Word Embeddings

### Insert The Helperfunctions

We are using the package [pradadata](https://github.com/sebastiansauer/pradadata) by @sebastian_sauer_2018_1996614. The data are licensed according to General Public License 3 (GLP-3).


```{r}
data("schimpwoerter", package = "pradadata")
data("sentiws", package = "pradadata")
data("wild_emojis", package = "pradadata")
source("C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/helper/helper_funs.R")
```


### rec3


```{r}
rec3 <-
recipe(c1 ~., data = select(d_train, text, c1, id)) %>%
update_role(id, new_role = "id") %>%
step_text_normalization(text) %>%
step_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>%
step_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>%
step_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>%
step_mutate(text_copy = text) %>%
step_textfeature(text_copy) %>%
step_tokenize(text) %>%
step_stopwords(text, language = "de", stopword_source = "snowball") %>%
step_stem(text) %>%
step_word_embeddings(text, embeddings = word_embedding_text)
```

```{r}
rec3_prep <- rec3 %>%
prep() %>%
recipes::bake(new_data = NULL)
```


## 5.4 Define Recipe - rec4 - TF-IDF

### rec4


```{r}
rec4 <-
recipe(c1 ~., data = select(d_train, text, c1, id)) %>%
update_role(id, new_role = "id") %>% 
step_text_normalization(text) %>%
step_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>%
step_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>%
step_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>%
step_mutate(text_copy = text) %>%
step_textfeature(text_copy) %>%
step_tokenize(text) %>%
step_stopwords(text, language = "de", stopword_source = "snowball") %>%
step_stem(text) %>%
step_tfidf(text)
```

```{r}
rec4_prep <- rec4 %>%
prep() %>%
recipes::bake(new_data = NULL)
```


# 6. Build Resamples

I decided to go with the v-fold-cross-validation as it is rather time-efficient. We have a large amount of data with extremely extensive recipes, so the run-time would be enormous, if we would try another resampling option, e.g. bootstrapping.


```{r}
folds <- vfold_cv(data = d_train,
v = 2,
repeats = 2,
strata = c1)
```


# 7. Build the Penalty-Grid


```{r}
lambda_grid <- grid_regular(penalty(), levels = 20)
```

