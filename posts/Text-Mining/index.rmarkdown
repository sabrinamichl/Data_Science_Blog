---
title: "Text Mining"
author: "Sabrina Michl"
date: "2023-01-27"
categories: [code, analysis]
bibliography: ref.bib
image: "image.jpg"
---


# 1. Preliminary Note

For this analysis we use the dataset from @data/0B5VML_2019 out of the zip archive @data/0B5VML/XIUWJ7_2019. The data are licensed according to Attribution 4.0 International (CC-BY-4.0). The used wordembeddings are from @grave2018learning. The data are licensed according to Attribution-ShareAlike 3.0 Unported (CC-BY-SA 3.0). The picture, that is used is from @bildquelle.

# 2. Load The Packages


```{r output=FALSE}
library(tidyverse)
library(rio)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(lsa)
library(discrim)
library(naivebayes)
library(tictoc)
library(fastrtext)
library(remoji)
library(tokenizers)
```


# 3. Load Dataset And Minor Changes

## 3.1 Train Dataset


```{r output=FALSE}
d_train <- read_tsv("C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.training.txt", col_names = FALSE)
```


### Rename Columns


```{r}
names(d_train) <- c("text", "c1", "c2")
```


### Add ID Column


```{r}
d_train <- d_train %>%
mutate(id = row_number()) %>%
select(id, everything())
```


## 3.2 Test Dataset


```{r output=FALSE}
d_test <- read_tsv("C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.test.txt", col_names = FALSE)
```


### Rename Columns


```{r}
names(d_test) <- c("text", "c1", "c2")
```


### Add ID Column


```{r}
d_test <- d_test %>%
mutate(id = row_number()) %>%
select(id, everything())
```


# 4. Explore Dataset


```{r}
train_toc <- d_train %>%
unnest_tokens(output = token, input = text)
train_toc
```


> First we tokenize the dataset d_train. \## Insert `Stopwords_de`


```{r}
data(stopwords_de, package = "lsa")
stopwords_de <- tibble(word = stopwords_de)
stopwords_de <- stopwords_de %>%
rename(token = word)
```


> After that we use the stopwords_de to `anti_join` this with train_toc.


```{r}
train_toc2 <- train_toc %>%
anti_join(stopwords_de)
```


## Show The Important Words


```{r}
train_toc2 <- train_toc2 %>%
count(token, sort = TRUE)
```


### Plot


```{r}
train_toc2 %>%
slice_head(n=20) %>%
ggplot()+
aes(y=reorder(factor(token), n), x = n, color = token)+
geom_col(aes(fill = token, alpha = 2.5)) +
ggtitle("The most used words") +
ylab("token")+
xlab("quantity")+
theme_minimal()+
theme(legend.position = "none")
```


> We see that, to most used word is "lbr". We could inspect the dataset way deeper, e. g. do a manual sentimentanalysis or do a lemmatization or stem the words. But we will have a look to these types in the different machine learning algorithm now.

# 5. Preparation

## 5.1 Define Recipe - rec1 - TF-IDF


```{r}
rec1 <-
recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>%
update_role(id, new_role = "id") %>%
step_tokenize(text) %>%
step_stopwords(text, language = "de", stopword_source = "snowball") %>%
step_stem(text) %>%
step_tfidf(text) %>%
step_normalize(all_numeric_predictors())
rec1
```


### Prep & Bake - rec1


```{r}
rec1_prep <- rec1 %>%
prep() %>%
recipes::bake(new_data = NULL)
```


## 5.2 Define Recipe - rec2 - word embedding

After fitting all the models I have decided to not fit the easy word embedding recipe (rec2)! The reason for this decision is, because of the analysis run-time! We have a large amount of data and extensive recipes, so when I raise the resamples for the prediction, R Studio quits the session.

-   [training:]{.underline} v = 3, repeats = 1
-   [prediction:]{.underline} v = 5, repeats = 2


```{r}
#rec2 <-
#recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>%
#update_role(id, new_role = "id") %>%
#step_tokenize(text) %>%
#step_stopwords(text, language = "de", stopword_source = "snowball") %>%
#step_word_embeddings(text, embeddings = word_embedding_text)
```


### Insert The Predefined List


```{r}
out_file_model <- "C:/Users/sapi-/OneDrive - Hochschule fÃ¼r Angewandte Wissenschaften Ansbach/Desktop/AWM/angewandte Wirtschats- und Medienpsychologie/5. Semester/Word_Embedding/de.300.bin"
```

```{r}
file.exists(out_file_model)
```

```{r}
fasttext_model <- load_model(out_file_model)
dictionary <- get_dictionary(fasttext_model)
get_word_vectors(fasttext_model, c("menschen")) %>% `[`(1:10)
```

```{r}
print(head(dictionary, 10))
```

```{r}
word_embedding_text <- tibble(word = dictionary)
```

```{r}
options(mc.cores = parallel::detectCores())
words_vecs <- get_word_vectors(fasttext_model)
```

```{r output=FALSE}
word_embedding_text <-
word_embedding_text %>%
bind_cols(words_vecs)
```

```{r}
names(word_embedding_text) <- c("word", paste0("v", sprintf("%03d", 1:301)))
```


## 5.3 Define Recipe - rec3 - Word Embeddings

### Insert the Helperfunctions

We are using the package \[pradadata\] (https://github.com/sebastiansauer/pradadata) from @sebastian_sauer_2018_1996614. The data are licensed according to General Public License 3 (GLP-3).


```{r}
data("schimpwoerter", package = "pradadata")
data("sentiws", package = "pradadata")
data("wild_emojis", package = "pradadata")
source("C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/helper/helper_funs.R")
```


### rec3


```{r}
rec3 <-
recipe(c1 ~., data = select(d_train, text, c1, id)) %>%
update_role(id, new_role = "id") %>%
step_text_normalization(text) %>%
step_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>%
step_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>%
step_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>%
step_mutate(text_copy = text) %>%
step_textfeature(text_copy) %>%
step_tokenize(text) %>%
step_stopwords(text, language = "de", stopword_source = "snowball") %>%
step_stem(text) %>%
step_word_embeddings(text, embeddings = word_embedding_text)
```

```{r}
rec3_prep <- rec3 %>%
prep() %>%
recipes::bake(new_data = NULL)
```


## 5.4 Define Recipe - rec4 - TF-IDF

### rec4


```{r}
rec4 <-
recipe(c1 ~., data = select(d_train, text, c1, id)) %>%
update_role(id, new_role = "id") %>%
step_text_normalization(text) %>%
step_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>%
step_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>%
step_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>%
step_mutate(text_copy = text) %>%
step_textfeature(text_copy) %>%
step_tokenize(text) %>%
step_stopwords(text, language = "de", stopword_source = "snowball") %>%
step_stem(text) %>%
step_tfidf(text)
```

```{r}
rec4_prep <- rec4 %>%
prep() %>%
recipes::bake(new_data = NULL)
```


# 6. Build Resamples

I have decided to go with the V-Fold-Cross-Validation, because of the time. We have a large amount of data with extremely extensive recipes, so the run-time would be enormous, if we would try another option of resampling, e. g. bootstrapping.


```{r}
folds <- vfold_cv(data = d_train,
v = 5,
repeats = 2,
strata = c1)
```


# 7. Build the Penalty-Grid


```{r}
lambda_grid <- grid_regular(penalty(), levels = 30)
```


# 8. Build the Models

## 8.1 Null Model


```{r}
tic()
```

```{r}
mod0 <- null_model() %>%
set_engine("parsnip") %>%
set_mode("classification")
```


### Define The Workflow


```{r}
wf0 <- workflow() %>%
add_recipe(rec1) %>%
add_model(mod0)
```


### Resampling & Model Quality


```{r}
options(mc.cores = parallel::detectCores())
tic()
fit0 <- fit_resamples(
wf0,
folds,
control =control_resamples(save_pred = TRUE)
)
toc()
```

```{r}
performance0 <- collect_metrics(fit0)
performance0
```

```{r}
preds0 <- collect_predictions(fit0)
preds0 %>%
group_by(id) %>%
roc_curve(truth = c1, .pred_OFFENSE) %>%
autoplot()
```

```{r}
conf_mat_resampled(fit0, tidy = FALSE) %>%
autoplot(type = "heatmap")
```


## 8.2 Lasso-L1 With TF-IDF

### L1-Model


```{r}
l1_82_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet") %>%
set_mode("classification")
l1_82_mod
```


### Define The Workflow


```{r}
l1_82_wf <-workflow() %>%
add_recipe(rec1) %>%
add_model(l1_82_mod)
l1_82_wf
```


### Resampling & Model Quality


```{r}
options(mc.cores = parallel::detectCores())
l1_82_wf_fit <- tune_grid(
l1_82_wf,
folds,
grid = lambda_grid,
control = control_resamples(save_pred = TRUE)
)
```


### Model Performance


```{r}
l1_82_wf_fit_performance <- collect_metrics(l1_82_wf_fit)
l1_82_wf_fit_performance
```

```{r}
l1_82_wf_fit_preds <- collect_predictions(l1_82_wf_fit)
```

```{r}
l1_82_wf_fit_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```

```{r}
autoplot(l1_82_wf_fit)
```


### Select The Best


```{r}
chosen_auc_l1_82_wf_fit <-
l1_82_wf_fit %>%
select_by_one_std_err(metric = "roc_auc", -penalty)
chosen_auc_l1_82_wf_fit
```


## 8.3 Ridge-Regression-L2 With TF-IDF

### L2-Model


```{r}
l2_83_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%
set_engine("glmnet") %>%
set_mode("classification")
l2_83_mod
```


### Define The Workflow


```{r}
l2_83_wf <-workflow() %>%
add_recipe(rec1) %>%
add_model(l2_83_mod)
l2_83_wf
```


### Resampling & Model Quality


```{r}
options(mc.cores = parallel::detectCores())
l2_83_wf_fit <- tune_grid(
l2_83_wf,
folds,
grid = lambda_grid,
control = control_resamples(save_pred = TRUE)
)
```

```{r}
collect_metrics(l2_83_wf_fit)
```


### Model Performance


```{r}
l2_83_wf_fit_performance <- collect_metrics(l2_83_wf_fit)
l2_83_wf_fit_performance
```

```{r}
l2_83_wf_fit_preds <- collect_predictions(l2_83_wf_fit)
```

```{r}
l2_83_wf_fit_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```


### Select The Best


```{r}
chosen_auc_l2_83_wf_fit <-
l2_83_wf_fit %>%
select_by_one_std_err(metric = "roc_auc", -penalty)
chosen_auc_l2_83_wf_fit
```


## 8.4 Lasso-L1 With Word Embeddings

### L1-Model


```{r}
l1_84_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet") %>%
set_mode("classification")
l1_84_mod
```


### Define The Workflow


```{r}
l1_84_wf <- workflow() %>%
add_recipe(rec3) %>%
add_model(l1_84_mod)
l1_84_wf
```


### Resampling & Model Quality


```{r}
options(mc.cores = parallel::detectCores())
l1_84_wf_fit <- tune_grid(
l1_84_wf,
folds,
grid = lambda_grid,
control = control_resamples(save_pred = TRUE)
)
```

```{r}
collect_metrics(l1_84_wf_fit)
```


### Model Performance


```{r}
l1_84_wf_fit_performance <- collect_metrics(l1_84_wf_fit)
l1_84_wf_fit_performance
```

```{r}
l1_84_wf_fit_preds <- collect_predictions(l1_84_wf_fit)
```

```{r}
l1_84_wf_fit_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```


### Select The Best


```{r}
chosen_auc_l1_84_wf_fit <-
l1_84_wf_fit %>%
select_by_one_std_err(metric = "roc_auc", -penalty)
chosen_auc_l1_84_wf_fit
```


## 8.5 Ridge-Regression-L2 with TF-IDF

### L2-Model


```{r}
l2_85_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%
set_engine("glmnet") %>%
set_mode("classification")
l2_85_mod
```


### Define The Workflow


```{r}
l2_85_wf <-workflow() %>%
add_recipe(rec3) %>%
add_model(l2_85_mod)
l2_85_wf
```


### Resampling & Model Quality


```{r}
options(mc.cores = parallel::detectCores())
l2_85_wf_fit <- tune_grid(
l2_85_wf,
folds,
grid = lambda_grid,
control = control_resamples(save_pred = TRUE)
)
```


### Model Performance


```{r}
l2_85_wf_performance <- collect_metrics(l2_85_wf_fit)
l2_85_wf_performance
```

```{r}
l2_85_wf_fit_preds <- collect_predictions(l2_85_wf_fit)
```

```{r}
l2_85_wf_fit_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```


### Show The Best


```{r}
show_best(l2_85_wf_fit)
```


### Select The Best


```{r}
chosen_auc_l2_85_wf_fit <-
l2_85_wf_fit %>%
select_by_one_std_err(metric = "roc_auc", -penalty)
chosen_auc_l2_85_wf_fit
```


## 8.6 Lasso-L1 With TF-IDF

### L1-Model


```{r}
l1_86_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet") %>%
set_mode("classification")
l1_86_mod
```


### Define The Workflow


```{r}
l1_86_wf <-workflow() %>%
add_recipe(rec4) %>%
add_model(l1_86_mod)
l1_86_wf
```


### Resampling & Model Quality


```{r}
options(mc.cores = parallel::detectCores())
l1_86_wf_fit <- tune_grid(
l1_86_wf,
folds,
grid = lambda_grid,
control = control_resamples(save_pred = TRUE)
)
```


### Model Performance


```{r}
l1_86_wf_performance <- collect_metrics(l1_86_wf_fit)
l1_86_wf_performance
```

```{r}
l1_86_wf_fit_preds <- collect_predictions(l1_86_wf_fit)
```

```{r}
l1_86_wf_fit_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```


### Show The Best


```{r}
select_best(l1_86_wf_fit)
```


### Select The Best


```{r}
chosen_auc_l1_86_wf_fit <-
l1_86_wf_fit %>%
select_by_one_std_err(metric = "roc_auc", -penalty)
chosen_auc_l1_86_wf_fit
```


## 8.7 Ridge-Regression-L2 With TF-IDF

### L2-Model


```{r}
l2_87_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%
set_engine("glmnet") %>%
set_mode("classification")
l2_87_mod
```


### Define The Workflow


```{r}
l2_87_wf <-workflow() %>%
add_recipe(rec4) %>%
add_model(l2_87_mod)
l2_87_wf
```


### Resampling & Model Quality


```{r}
options(mc.cores = parallel::detectCores())
l2_87_wf_fit <- tune_grid(
l2_87_wf,
folds,
grid = lambda_grid,
control = control_resamples(save_pred = TRUE)
)
```


### Model Performance


```{r}
l2_87_wf_performance <- collect_metrics(l2_87_wf_fit)
l2_87_wf_performance
```

```{r}
l2_87_wf_fit_preds <- collect_predictions(l2_87_wf_fit)
```

```{r}
l2_87_wf_fit_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = c1, .pred_OFFENSE) %>% 
  autoplot()
```


### Select The Best


```{r}
chosen_auc_l2_87_wf_fit <-
l2_87_wf_fit %>%
select_by_one_std_err(metric = "roc_auc", -penalty)
chosen_auc_l2_87_wf_fit
```

```{r}
toc()
```


# 9. Predictions

