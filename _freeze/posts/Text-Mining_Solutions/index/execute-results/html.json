{
  "hash": "877f92ef4fbdf645263fa3fe6b896617",
  "result": {
    "markdown": "---\ntitle: \"Text Mining Solution\"\nauthor: \"Sabrina Michl\"\ndate: \"2023-01-30\"\ncategories: [code, analysis]\nbibliography: ref.bib\n---\n\n\n# 1. Preliminary Note\n\nFor this analysis we use the dataset from @data/0B5VML_2019 out of the zip archive @data/0B5VML/XIUWJ7_2019. The data are licensed according to Attribution 4.0 International (CC-BY-4.0). The used word embeddings are from @grave2018learning. The data are licensed according to Attribution-ShareAlike 3.0 Unported (CC-BY-SA 3.0).\n\n# 2. Explaination\nUnfortunately it was not possible to predict in the analysis blogpost.\nThe code worked and the system was ready to predict.\nBut every time I ran the prediction code I got the same error message in the end \"Error: cannot allocate vector of size 672.8 Mb\".\nI tried to minimize the complexity of the code, but after 3 new trials with bad luck, I decided to compute the predictions in this separate blogpost.\n\n# 3. Load The Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(lsa)\nlibrary(discrim)\nlibrary(naivebayes)\nlibrary(tictoc)\nlibrary(fastrtext)\nlibrary(remoji)\nlibrary(tokenizers)\n```\n:::\n\n\n# 4. Load Dataset And Minor Changes\n\n## 4.1 Train Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.training.txt\", col_names = FALSE)\n```\n:::\n\n\n### Rename Columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_train) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\n### Add ID Column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <- d_train %>%\nmutate(id = row_number()) %>%\nselect(id, everything())\n```\n:::\n\n\n## 4.2 Test Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.test.txt\", col_names = FALSE)\n```\n:::\n\n\n### Rename Columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_test) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\n### Add ID Column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <- d_test %>%\nmutate(id = row_number()) %>%\nselect(id, everything())\n```\n:::\n\n\n# 5. Insert The Predefined List\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_file_model <- \"C:/Users/sapi-/OneDrive - Hochschule fÃ¼r Angewandte Wissenschaften Ansbach/Desktop/AWM/angewandte Wirtschats- und Medienpsychologie/5. Semester/Word_Embedding/de.300.bin\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.exists(out_file_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfasttext_model <- load_model(out_file_model)\ndictionary <- get_dictionary(fasttext_model)\nget_word_vectors(fasttext_model, c(\"menschen\")) %>% `[`(1:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] -0.043737594 -0.033647023 -0.016398411  0.037433818  0.029863771\n [6] -0.008217440  0.002691153 -0.027484305 -0.058012061  0.004103063\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(head(dictionary, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \",\"    \".\"    \"</s>\" \"und\"  \"der\"  \":\"    \"die\"  \"\\\"\"   \")\"    \"(\"   \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nword_embedding_text <- tibble(word = dictionary)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nwords_vecs <- get_word_vectors(fasttext_model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nword_embedding_text <-\nword_embedding_text %>%\nbind_cols(words_vecs)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(word_embedding_text) <- c(\"word\", paste0(\"v\", sprintf(\"%03d\", 1:301)))\n```\n:::\n\n\n# 6. Insert The Helperfunctions\n\nWe are using the package [pradadata](https://github.com/sebastiansauer/pradadata) by @sebastian_sauer_2018_1996614. The data are licensed according to General Public License 3 (GLP-3).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"schimpwoerter\", package = \"pradadata\")\ndata(\"sentiws\", package = \"pradadata\")\ndata(\"wild_emojis\", package = \"pradadata\")\nsource(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/helper/helper_funs.R\")\n```\n:::\n\n\n# 7. Define Recipe - rec4 - TF-IDF\n\n::: {.cell}\n\n```{.r .cell-code}\nrec4 <-\nrecipe(c1 ~., data = select(d_train, text, c1, id)) %>%\nupdate_role(id, new_role = \"id\") %>% \nstep_text_normalization(text) %>%\nstep_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>%\nstep_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>%\nstep_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>%\nstep_mutate(text_copy = text) %>%\nstep_textfeature(text_copy) %>%\nstep_tokenize(text) %>%\nstep_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\nstep_stem(text) %>%\nstep_tfidf(text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrec4_prep <- rec4 %>%\nprep() %>%\nrecipes::bake(new_data = NULL)\n```\n:::\n\n\n# 8. Build Resamples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfolds <- vfold_cv(data = d_train,\nv = 3,\nrepeats = 1,\nstrata = c1)\n```\n:::\n\n\n# 9. Build the Penalty-Grid\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda_grid <- grid_regular(penalty(), levels = 20)\n```\n:::\n\n\n## 10. Lasso-L1 With TF-IDF\n\n### L1-Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_86_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl1_86_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define The Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_86_wf <-workflow() %>%\nadd_recipe(rec4) %>%\nadd_model(l1_86_mod)\nl1_86_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\ntic()\nl1_86_wf_fit <- tune_grid(\nl1_86_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'stringi' wurde unter R Version 4.1.2 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'textfeatures' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'stopwords' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'glmnet' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'Matrix' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n385.5 sec elapsed\n```\n:::\n:::\n\n\n### Model Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_86_wf_performance <- collect_metrics(l1_86_wf_fit)\nl1_86_wf_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 40 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.746     3 0.00265 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.777     3 0.00357 Preprocessor1_Model01\n 3 3.36e-10 accuracy binary     0.746     3 0.00265 Preprocessor1_Model02\n 4 3.36e-10 roc_auc  binary     0.777     3 0.00357 Preprocessor1_Model02\n 5 1.13e- 9 accuracy binary     0.746     3 0.00265 Preprocessor1_Model03\n 6 1.13e- 9 roc_auc  binary     0.777     3 0.00357 Preprocessor1_Model03\n 7 3.79e- 9 accuracy binary     0.746     3 0.00265 Preprocessor1_Model04\n 8 3.79e- 9 roc_auc  binary     0.777     3 0.00357 Preprocessor1_Model04\n 9 1.27e- 8 accuracy binary     0.746     3 0.00265 Preprocessor1_Model05\n10 1.27e- 8 roc_auc  binary     0.777     3 0.00357 Preprocessor1_Model05\n# ... with 30 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_86_wf_fit_preds <- collect_predictions(l1_86_wf_fit)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_86_wf_fit_preds %>% \n  group_by(id) %>% \n  roc_curve(truth = c1, .pred_OFFENSE) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n### Select The Best\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l1_86_wf_fit <-\nl1_86_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l1_86_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1 0.00785 roc_auc binary     0.782     3 0.00708 Preprocessor1_Mod~ 0.782  0.775\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}