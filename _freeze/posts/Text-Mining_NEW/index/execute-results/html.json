{
  "hash": "ebf66db6109daa9cf9784d69ec1e55ea",
  "result": {
    "markdown": "---\ntitle: \"Text Mining Analysis\"\nauthor: \"Sabrina Michl\"\ndate: \"2023-01-30\"\ncategories: [code, analysis]\nbibliography: ref.bib\nimage: \"image.jpg\"\n---\n\n\n# 1. Preliminary Note & Definition\n\n## 1.1 Preliminary Note\n\nFor this analysis we use the dataset from @data/0B5VML_2019 out of the zip archive @data/0B5VML/XIUWJ7_2019. The data are licensed according to Attribution 4.0 International (CC-BY-4.0).\n\nThe picture to this post from the welcome page, is from <a href=\"https://pixabay.com/de/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=7691355\">Gerd Altmann</a> at <a href=\"https://pixabay.com/de//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=7691355\">Pixabay</a>\n\n## 1.2 Definition of Hate Speech\n\nThe [United Nations](https://www.un.org/en/hate-speech/understanding-hate-speech/what-is-hate-speech) defines hate speech as: \"***any kind of communication** in speech, writing or behaviour, that **attacks** or uses **pejorative** or **discriminatory** language with reference to a person or a group on the basis of **who they are**, in other words, based on their religion, ethnicity, nationality, race, colour, descent, gender or other identity factor.\"*\n\nThe problem about hate speech is, that there is no universal definition that can be used.\n\nSo our research question is, if there is any possibility to predict hate speech by using tweets and how well is the prediction?\n\n# 2. Load The Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(lsa)\nlibrary(discrim)\nlibrary(naivebayes)\nlibrary(tictoc)\nlibrary(fastrtext)\nlibrary(remoji)\nlibrary(tokenizers)\n```\n:::\n\n\n# 3. Load Dataset And Minor Changes\n\n## 3.1 Train Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.training.txt\", col_names = FALSE)\n```\n:::\n\n\n### Rename Columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_train) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\n### Add ID Column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <- d_train %>%\nmutate(id = row_number()) %>%\nselect(id, everything())\n```\n:::\n\n\n# 4. Explore Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc <- d_train %>%\nunnest_tokens(output = token, input = text)\ntrain_toc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100,217 x 4\n      id c1    c2    token         \n   <int> <chr> <chr> <chr>         \n 1     1 OTHER OTHER corinnamilborn\n 2     1 OTHER OTHER liebe         \n 3     1 OTHER OTHER corinna       \n 4     1 OTHER OTHER wir           \n 5     1 OTHER OTHER würden        \n 6     1 OTHER OTHER dich          \n 7     1 OTHER OTHER gerne         \n 8     1 OTHER OTHER als           \n 9     1 OTHER OTHER moderatorin   \n10     1 OTHER OTHER für           \n# ... with 100,207 more rows\n```\n:::\n:::\n\n\n> First tokenize the dataset d_train.\n\n## 4.1 Insert `Stopwords_de`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(stopwords_de, package = \"lsa\")\nstopwords_de <- tibble(word = stopwords_de)\nstopwords_de <- stopwords_de %>%\nrename(token = word)\n```\n:::\n\n\n> After that we use the stopwords_de to `anti_join` this with train_toc dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 <- train_toc %>%\nanti_join(stopwords_de)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"token\"\n```\n:::\n:::\n\n\n## Show The Important Words\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 <- train_toc2 %>%\ncount(token, sort = TRUE)\n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 %>%\nslice_head(n=20) %>%\nggplot()+\naes(y=reorder(factor(token), n), x = n, color = token)+\ngeom_col(aes(fill = token, alpha = 2.5)) +\nggtitle(\"The Most Used Words\") +\nylab(\"Token\")+\nxlab(\"Quantity\")+\ntheme_minimal()+\ntheme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n> We see that the most used word is \"lbr\". We could inspect the dataset way deeper, e.g. do a manual sentimentanalysis, do a lemmatization or stem the words. But we will have a look at these processes in the different machine learning algorithms following now.\n\n# 5. Preparation\n\n## 5.1 Define Recipe - rec1 - TF-IDF\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec1 <-\nrecipe(c1 ~ ., data = select(d_train, text, c1, id)) %>%\nupdate_role(id, new_role = \"id\") %>%\nstep_tokenize(text) %>%\nstep_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\nstep_stem(text) %>%\nstep_tfidf(text) %>%\nstep_normalize(all_numeric_predictors())\nrec1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nStemming for text\nTerm frequency-inverse document frequency with text\nCentering and scaling for all_numeric_predictors()\n```\n:::\n:::\n\n\n### Prep & Bake - rec1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec1_prep <- rec1 %>%\nprep() %>%\nrecipes::bake(new_data = NULL)\n```\n:::\n\n\n## 5.2 Define Recipe - rec2 - word embedding\n\nDue to the long calculation time and the relatively poor roc_auc values (with the training resample - v = 2, repeats = 1) , I decided not to perform the calculation for the analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#rec2 <-\n#recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>%\n#update_role(id, new_role = \"id\") %>%\n#step_tokenize(text) %>%\n#step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\n#step_word_embeddings(text, embeddings = word_embedding_text)\n```\n:::\n\n\n## 5.3 Define Recipe - rec3 - Word Embeddings\n\n### Insert The Helperfunctions\n\nWe are using the package [pradadata](https://github.com/sebastiansauer/pradadata) by @sebastian_sauer_2018_1996614. The data are licensed according to General Public License 3 (GLP-3).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"schimpwoerter\", package = \"pradadata\")\ndata(\"sentiws\", package = \"pradadata\")\ndata(\"wild_emojis\", package = \"pradadata\")\nsource(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/helper/helper_funs.R\")\n```\n:::\n\n\n### Insert The Predefined Word Embedding List\n\nThe used word embeddings are from @grave2018learning. The data are licensed according to Attribution-ShareAlike 3.0 Unported (CC-BY-SA 3.0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_file_model <- \"C:/Users/sapi-/OneDrive - Hochschule für Angewandte Wissenschaften Ansbach/Desktop/AWM/angewandte Wirtschats- und Medienpsychologie/5. Semester/Word_Embedding/de.300.bin\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.exists(out_file_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfasttext_model <- load_model(out_file_model)\ndictionary <- get_dictionary(fasttext_model)\nget_word_vectors(fasttext_model, c(\"menschen\")) %>% `[`(1:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] -0.043737594 -0.033647023 -0.016398411  0.037433818  0.029863771\n [6] -0.008217440  0.002691153 -0.027484305 -0.058012061  0.004103063\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(head(dictionary, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \",\"    \".\"    \"</s>\" \"und\"  \"der\"  \":\"    \"die\"  \"\\\"\"   \")\"    \"(\"   \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nword_embedding_text <- tibble(word = dictionary)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nwords_vecs <- get_word_vectors(fasttext_model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nword_embedding_text <-\nword_embedding_text %>%\nbind_cols(words_vecs)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(word_embedding_text) <- c(\"word\", paste0(\"v\", sprintf(\"%03d\", 1:301)))\n```\n:::\n\n\n### rec3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec3 <-\nrecipe(c1 ~., data = select(d_train, text, c1, id)) %>%\nupdate_role(id, new_role = \"id\") %>%\nstep_text_normalization(text) %>%\nstep_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>%\nstep_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>%\nstep_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>%\nstep_mutate(text_copy = text) %>%\nstep_textfeature(text_copy) %>%\nstep_tokenize(text) %>%\nstep_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\nstep_stem(text) %>%\nstep_word_embeddings(text, embeddings = word_embedding_text)\n```\n:::\n\n\n### Prep & Bake - rec3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec3_prep <- rec3 %>%\nprep() %>%\nrecipes::bake(new_data = NULL)\n```\n:::\n\n\n## 5.4 Define Recipe - rec4 - TF-IDF\n\n### rec4\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec4 <-\nrecipe(c1 ~., data = select(d_train, text, c1, id)) %>%\nupdate_role(id, new_role = \"id\") %>% \nstep_text_normalization(text) %>%\nstep_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>%\nstep_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>%\nstep_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>%\nstep_mutate(text_copy = text) %>%\nstep_textfeature(text_copy) %>%\nstep_tokenize(text) %>%\nstep_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\nstep_stem(text) %>%\nstep_tfidf(text)\n```\n:::\n\n\n### Prep & Bake - rec4\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec4_prep <- rec4 %>%\nprep() %>%\nrecipes::bake(new_data = NULL)\n```\n:::\n\n\n# 6. Build Resamples\n\nI chose the v-fold-cross-validation as it is rather time-efficient. Other resampling methods are for example bootstrapping, leave-one-out-cross-validation or hold-out-cross-validation.\n\nFor the training I have used v = 2 and repeats = 1. The advantage is, you can train the dataset without such a long calculation time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfolds <- vfold_cv(data = d_train,\nv = 3,\nrepeats = 2,\nstrata = c1)\n```\n:::\n\n\n# 7. Build the Penalty-Grid\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda_grid <- grid_regular(penalty(), levels = 25)\n```\n:::\n\n\n# 8. Build the Models\n\n## 8.1 Null Model\n\n## 8.2 Lasso-L1 With TF-IDF\n\nAccording to the large amount of data, I decided to not run the `Null Model` and the `L1-TF-IDF with rec1`.\n\n## 8.3 Ridge-Regression-L2 With TF-IDF\n\n### L2-Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_83_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl2_83_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define The Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_83_wf <-workflow() %>%\nadd_recipe(rec1) %>%\nadd_model(l2_83_mod)\nl2_83_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n5 Recipe Steps\n\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nl2_83_wf_fit <- tune_grid(\nl2_83_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'stopwords' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'glmnet' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'Matrix' wurde unter R Version 4.1.3 erstellt\n```\n:::\n:::\n\n\n### Model Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_83_wf_fit_performance <- collect_metrics(l2_83_wf_fit)\nl2_83_wf_fit_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.710     6 0.00452 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.757     6 0.00336 Preprocessor1_Model01\n 3 2.61e-10 accuracy binary     0.710     6 0.00452 Preprocessor1_Model02\n 4 2.61e-10 roc_auc  binary     0.757     6 0.00336 Preprocessor1_Model02\n 5 6.81e-10 accuracy binary     0.710     6 0.00452 Preprocessor1_Model03\n 6 6.81e-10 roc_auc  binary     0.757     6 0.00336 Preprocessor1_Model03\n 7 1.78e- 9 accuracy binary     0.710     6 0.00452 Preprocessor1_Model04\n 8 1.78e- 9 roc_auc  binary     0.757     6 0.00336 Preprocessor1_Model04\n 9 4.64e- 9 accuracy binary     0.710     6 0.00452 Preprocessor1_Model05\n10 4.64e- 9 roc_auc  binary     0.757     6 0.00336 Preprocessor1_Model05\n# ... with 40 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_83_wf_fit_preds <- collect_predictions(l2_83_wf_fit)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_83_wf_fit_preds %>% \n  group_by(id) %>% \n  roc_curve(truth = c1, .pred_OFFENSE) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n### Select The Best\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l2_83_wf_fit <-\nl2_83_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l2_83_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1       1 roc_auc binary     0.758     6 0.00331 Preprocessor1_Mod~ 0.758  0.755\n```\n:::\n:::\n\n\n### Positive Predictive Value\n\n\n::: {.cell}\n\n```{.r .cell-code}\nppv(l2_83_wf_fit_preds, truth = factor(c1), estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 ppv     binary         0.644\n```\n:::\n:::\n\n\n## 8.4 Lasso-L1 With Word Embeddings\n\nLike the `Null Model` (8.1) and the `L1 with TF-IDF` (8.2) I decided to kick the `L1 with Word Embeddings` out of the analysis, because the results in the training phase were too poor, to use it in the prediction phase.\n\n## 8.5 Ridge-Regression-L2 with TF-IDF\n\n### L2-Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_85_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl2_85_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define The Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_85_wf <-workflow() %>%\nadd_recipe(rec3) %>%\nadd_model(l2_85_mod)\nl2_85_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_word_embeddings()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nl2_85_wf_fit <- tune_grid(\nl2_85_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'stringi' wurde unter R Version 4.1.2 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'textfeatures' wurde unter R Version 4.1.3 erstellt\n```\n:::\n:::\n\n\n### Model Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_85_wf_performance <- collect_metrics(l2_85_wf_fit)\nl2_85_wf_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.733     6 0.00388 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.755     6 0.00625 Preprocessor1_Model01\n 3 2.61e-10 accuracy binary     0.733     6 0.00388 Preprocessor1_Model02\n 4 2.61e-10 roc_auc  binary     0.755     6 0.00625 Preprocessor1_Model02\n 5 6.81e-10 accuracy binary     0.733     6 0.00388 Preprocessor1_Model03\n 6 6.81e-10 roc_auc  binary     0.755     6 0.00625 Preprocessor1_Model03\n 7 1.78e- 9 accuracy binary     0.733     6 0.00388 Preprocessor1_Model04\n 8 1.78e- 9 roc_auc  binary     0.755     6 0.00625 Preprocessor1_Model04\n 9 4.64e- 9 accuracy binary     0.733     6 0.00388 Preprocessor1_Model05\n10 4.64e- 9 roc_auc  binary     0.755     6 0.00625 Preprocessor1_Model05\n# ... with 40 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_85_wf_fit_preds <- collect_predictions(l2_85_wf_fit)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_85_wf_fit_preds %>% \n  group_by(id) %>% \n  roc_curve(truth = c1, .pred_OFFENSE) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n### Select The Best\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l2_85_wf_fit <-\nl2_85_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l2_85_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1   0.383 roc_auc binary     0.756     6 0.00830 Preprocessor1_Mod~ 0.762  0.754\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat_resampled(l2_85_wf_fit, tidy = FALSE, parameter = select_best(l2_85_wf_fit)) %>% \n  autoplot(type = \"heatmap\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n\n### Positive Predictive Value\n\n\n::: {.cell}\n\n```{.r .cell-code}\nppv(l2_85_wf_fit_preds, truth = factor(c1), estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 ppv     binary         0.656\n```\n:::\n:::\n\n\n## 8.6 Lasso-L1 With TF-IDF\n\n### L1-Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_86_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl1_86_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define The Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_86_wf <-workflow() %>%\nadd_recipe(rec4) %>%\nadd_model(l1_86_mod)\nl1_86_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\ntic()\nl1_86_wf_fit <- tune_grid(\nl1_86_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n771.11 sec elapsed\n```\n:::\n:::\n\n\n### Model Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_86_wf_performance <- collect_metrics(l1_86_wf_fit)\nl1_86_wf_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.744     6 0.00118 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.774     6 0.00369 Preprocessor1_Model01\n 3 2.61e-10 accuracy binary     0.744     6 0.00118 Preprocessor1_Model02\n 4 2.61e-10 roc_auc  binary     0.774     6 0.00369 Preprocessor1_Model02\n 5 6.81e-10 accuracy binary     0.744     6 0.00118 Preprocessor1_Model03\n 6 6.81e-10 roc_auc  binary     0.774     6 0.00369 Preprocessor1_Model03\n 7 1.78e- 9 accuracy binary     0.744     6 0.00118 Preprocessor1_Model04\n 8 1.78e- 9 roc_auc  binary     0.774     6 0.00369 Preprocessor1_Model04\n 9 4.64e- 9 accuracy binary     0.744     6 0.00118 Preprocessor1_Model05\n10 4.64e- 9 roc_auc  binary     0.774     6 0.00369 Preprocessor1_Model05\n# ... with 40 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_86_wf_fit_preds <- collect_predictions(l1_86_wf_fit)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_86_wf_fit_preds %>% \n  group_by(id) %>% \n  roc_curve(truth = c1, .pred_OFFENSE) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-50-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat_resampled(l1_86_wf_fit, tidy = FALSE, parameter = select_best(l1_86_wf_fit)) %>% \n  autoplot(type = \"heatmap\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\n### Select The Best\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l1_86_wf_fit <-\nl1_86_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l1_86_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1 0.00825 roc_auc binary     0.783     6 0.00583 Preprocessor1_Mod~ 0.783  0.777\n```\n:::\n:::\n\n\n### Positive Predictive Value\n\n\n::: {.cell}\n\n```{.r .cell-code}\nppv(l1_86_wf_fit_preds, truth = factor(c1), estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 ppv     binary         0.672\n```\n:::\n:::\n\n\n## 8.7 Ridge-Regression-L2 With TF-IDF\n\n### L2-Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_87_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl2_87_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define The Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_87_wf <-workflow() %>%\nadd_recipe(rec4) %>%\nadd_model(l2_87_mod)\nl2_87_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nl2_87_wf_fit <- tune_grid(\nl2_87_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n\n### Model Performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_87_wf_performance <- collect_metrics(l2_87_wf_fit)\nl2_87_wf_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.711     6 0.00322 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.771     6 0.00387 Preprocessor1_Model01\n 3 2.61e-10 accuracy binary     0.711     6 0.00322 Preprocessor1_Model02\n 4 2.61e-10 roc_auc  binary     0.771     6 0.00387 Preprocessor1_Model02\n 5 6.81e-10 accuracy binary     0.711     6 0.00322 Preprocessor1_Model03\n 6 6.81e-10 roc_auc  binary     0.771     6 0.00387 Preprocessor1_Model03\n 7 1.78e- 9 accuracy binary     0.711     6 0.00322 Preprocessor1_Model04\n 8 1.78e- 9 roc_auc  binary     0.771     6 0.00387 Preprocessor1_Model04\n 9 4.64e- 9 accuracy binary     0.711     6 0.00322 Preprocessor1_Model05\n10 4.64e- 9 roc_auc  binary     0.771     6 0.00387 Preprocessor1_Model05\n# ... with 40 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_87_wf_fit_preds <- collect_predictions(l2_87_wf_fit)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_87_wf_fit_preds %>% \n  group_by(id) %>% \n  roc_curve(truth = c1, .pred_OFFENSE) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\n\n### Select The Best\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l2_87_wf_fit <-\nl2_87_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l2_87_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1       1 roc_auc binary     0.771     6 0.00387 Preprocessor1_Mod~ 0.771  0.767\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat_resampled(l2_87_wf_fit, tidy = FALSE, parameter = select_best(l2_87_wf_fit)) %>% \n  autoplot(type = \"heatmap\") \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-61-1.png){width=672}\n:::\n:::\n\n\n### Positive Predictive Value\n\n\n::: {.cell}\n\n```{.r .cell-code}\nppv(l2_87_wf_fit_preds, truth = factor(c1), estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 ppv     binary         0.690\n```\n:::\n:::\n\n\n# 9. Predictions\n\nYou will find them [here](https://world-of-datascience.netlify.app/posts/text-mining_solutions/)! 😊\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}