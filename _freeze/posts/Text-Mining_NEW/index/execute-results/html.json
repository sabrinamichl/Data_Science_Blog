{
  "hash": "e603e09924d17aa0a46cece665fe0ea8",
  "result": {
    "markdown": "---\ntitle: \"Text Mining\"\nauthor: \"Sabrina Michl\"\ndate: \"2023-01-31\"\ncategories: [code, analysis]\nbibliography: ref.bib\nimage: \"image.jpg\"\n---\n\n\n# 1. Preliminary Note\n\nFor this analysis we use the dataset from @data/0B5VML_2019 out of the zip archive @data/0B5VML/XIUWJ7_2019. The data are licensed according to Attribution 4.0 International (CC-BY-4.0). The used wordembeddings are from @grave2018learning. The data are licensed according to Attribution-ShareAlike 3.0 Unported (CC-BY-SA 3.0).\n\nThe picture to this post from the welcome page, is from <a href=\"https://pixabay.com/de/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=7691355\">Gerd Altmann</a> at <a href=\"https://pixabay.com/de//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=7691355\">Pixabay</a>\n\n# 2. Load The Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(lsa)\nlibrary(discrim)\nlibrary(naivebayes)\nlibrary(tictoc)\nlibrary(fastrtext)\nlibrary(remoji)\nlibrary(tokenizers)\n```\n:::\n\n\n# 3. Load Dataset And Minor Changes\n\n## 3.1 Train Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.training.txt\", col_names = FALSE)\n```\n:::\n\n\n### Rename Columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_train) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\n### Add ID Column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <- d_train %>%\nmutate(id = row_number()) %>%\nselect(id, everything())\n```\n:::\n\n\n## 3.2 Test Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.test.txt\", col_names = FALSE)\n```\n:::\n\n\n### Rename Columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_test) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\n### Add ID Column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <- d_test %>%\nmutate(id = row_number()) %>%\nselect(id, everything())\n```\n:::\n\n\n# 4. Explore Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc <- d_train %>%\nunnest_tokens(output = token, input = text)\ntrain_toc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100,217 x 4\n      id c1    c2    token         \n   <int> <chr> <chr> <chr>         \n 1     1 OTHER OTHER corinnamilborn\n 2     1 OTHER OTHER liebe         \n 3     1 OTHER OTHER corinna       \n 4     1 OTHER OTHER wir           \n 5     1 OTHER OTHER würden        \n 6     1 OTHER OTHER dich          \n 7     1 OTHER OTHER gerne         \n 8     1 OTHER OTHER als           \n 9     1 OTHER OTHER moderatorin   \n10     1 OTHER OTHER für           \n# ... with 100,207 more rows\n```\n:::\n:::\n\n\n> First we tokenize the dataset d_train. \\## Insert `Stopwords_de`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(stopwords_de, package = \"lsa\")\nstopwords_de <- tibble(word = stopwords_de)\nstopwords_de <- stopwords_de %>%\nrename(token = word)\n```\n:::\n\n\n> After that we use the stopwords_de to `anti_join` this with train_toc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 <- train_toc %>%\nanti_join(stopwords_de)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"token\"\n```\n:::\n:::\n\n\n## Show The Important Words\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 <- train_toc2 %>%\ncount(token, sort = TRUE)\n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 %>%\nslice_head(n=20) %>%\nggplot()+\naes(y=reorder(factor(token), n), x = n, color = token)+\ngeom_col(aes(fill = token, alpha = 2.5)) +\nggtitle(\"The most used words\") +\nylab(\"token\")+\nxlab(\"quantity\")+\ntheme_minimal()+\ntheme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}