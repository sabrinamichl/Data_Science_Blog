{
  "hash": "00d3f56cb9a4bcf0b7134acae715f7f9",
  "result": {
    "markdown": "---\ntitle: \"Datenauswertung \"\nauthor: \"Sabrina Michl\"\ndate: \"2023-01-25\"\ncategories: [code, analysis]\nbibliography: ref.bib\nimage: image.jpg\n---\n\n\n# 1. Preliminary note\n\nFor this analysis we use the dataset from @data/0B5VML_2019 out of the zip archive @data/0B5VML/XIUWJ7_2019. The data are licensed according to Attribution 4.0 International (CC-BY-4.0).\n\nThe used wordembeddings are from @grave2018learning. The data are licensed according to Attribution-ShareAlike 3.0 Unported (CC-BY-SA 3.0).\n\nThe picture, that is used is from @bildquelle.\n\n# 2. Load the packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(lsa)\nlibrary(discrim)\nlibrary(naivebayes)\nlibrary(tictoc)\nlibrary(fastrtext)\nlibrary(remoji)\nlibrary(tokenizers)\n```\n:::\n\n\n# 3. Load dataset and minor changes\n\n## Train dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.training.txt\", col_names = FALSE)\n```\n:::\n\n\n### Rename columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_train) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\n### Add ID column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <- d_train %>% \n  mutate(id = row_number()) %>% \n  select(id, everything())\n```\n:::\n\n\n## Test dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.test.txt\", col_names = FALSE)\n```\n:::\n\n\n### Rename columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_test) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\n### Add ID column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <- d_test %>% \n  mutate(id = row_number()) %>% \n  select(id, everything())\n```\n:::\n\n\n# 4. Explore dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc <- d_train %>% \n  unnest_tokens(output = token, input = text)\ntrain_toc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100,217 x 4\n      id c1    c2    token         \n   <int> <chr> <chr> <chr>         \n 1     1 OTHER OTHER corinnamilborn\n 2     1 OTHER OTHER liebe         \n 3     1 OTHER OTHER corinna       \n 4     1 OTHER OTHER wir           \n 5     1 OTHER OTHER würden        \n 6     1 OTHER OTHER dich          \n 7     1 OTHER OTHER gerne         \n 8     1 OTHER OTHER als           \n 9     1 OTHER OTHER moderatorin   \n10     1 OTHER OTHER für           \n# ... with 100,207 more rows\n```\n:::\n:::\n\n\n> First we tokenize the dataset d_train.\n\n## Insert stopwords_de\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(stopwords_de, package = \"lsa\")\nstopwords_de <- tibble(word = stopwords_de)\nstopwords_de <- stopwords_de %>% \n  rename(token = word)\n```\n:::\n\n\n> After that we use the stopwords_de to `anti_join` this with train_toc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 <- train_toc %>% \n  anti_join(stopwords_de)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"token\"\n```\n:::\n:::\n\n\n## Show the important words\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 <- train_toc2 %>% \n  count(token, sort = TRUE) \n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 %>% \n  slice_head(n=20) %>%  \n  ggplot()+\n  aes(y=reorder(factor(token), n), x = n, color = token)+\n  geom_col(aes(fill = token, alpha = 2.5)) +\n  ggtitle(\"The most used words\") +\n  ylab(\"token\")+\n  xlab(\"quantity\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}