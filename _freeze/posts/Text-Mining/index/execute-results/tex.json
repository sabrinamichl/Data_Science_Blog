{
  "hash": "e8d472117a46f086b17c1d53140042ea",
  "result": {
    "markdown": "---\ntitle: \"Text Mining\"\nauthor: \"Sabrina Michl\"\ndate: \"2023-01-25\"\ncategories: [code, analysis]\nbibliography: ref.bib\nimage: image.jpg\nformat:\n  pdf: \n   toc: true\n   number_section: true\n   colorlinks: true\n---\n\n\n# Preliminary Note\n\nFor this analysis we use the dataset from @data/0B5VML_2019 out of the zip archive @data/0B5VML/XIUWJ7_2019. The data are licensed according to Attribution 4.0 International (CC-BY-4.0).\n\nThe used wordembeddings are from @grave2018learning. The data are licensed according to Attribution-ShareAlike 3.0 Unported (CC-BY-SA 3.0).\n\nThe picture, that is used is from @bildquelle.\n\n# Load The Packages\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(textrecipes)\nlibrary(lsa)\nlibrary(discrim)\nlibrary(naivebayes)\nlibrary(tictoc)\nlibrary(fastrtext)\nlibrary(remoji)\nlibrary(tokenizers)\n```\n:::\n\n\n# Load Dataset And Minor Changes\n## Train Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.training.txt\", col_names = FALSE)\n```\n:::\n\n\n### Rename Columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_train) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\n### Add ID Column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <- d_train %>% \n  mutate(id = row_number()) %>% \n  select(id, everything())\n```\n:::\n\n\n## Test Dataset\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.test.txt\", col_names = FALSE)\n```\n:::\n\n\n### Rename Columns\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(d_test) <- c(\"text\", \"c1\", \"c2\")\n```\n:::\n\n\n### Add ID Column\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <- d_test %>% \n  mutate(id = row_number()) %>% \n  select(id, everything())\n```\n:::\n\n\n# Explore Dataset\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc <- d_train %>% \n  unnest_tokens(output = token, input = text)\ntrain_toc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100,217 x 4\n      id c1    c2    token         \n   <int> <chr> <chr> <chr>         \n 1     1 OTHER OTHER corinnamilborn\n 2     1 OTHER OTHER liebe         \n 3     1 OTHER OTHER corinna       \n 4     1 OTHER OTHER wir           \n 5     1 OTHER OTHER würden        \n 6     1 OTHER OTHER dich          \n 7     1 OTHER OTHER gerne         \n 8     1 OTHER OTHER als           \n 9     1 OTHER OTHER moderatorin   \n10     1 OTHER OTHER für           \n# ... with 100,207 more rows\n```\n:::\n:::\n\n\n> First we tokenize the dataset d_train.\n\n## Insert `Stopwords_de`\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(stopwords_de, package = \"lsa\")\nstopwords_de <- tibble(word = stopwords_de)\nstopwords_de <- stopwords_de %>% \n  rename(token = word)\n```\n:::\n\n\n> After that we use the stopwords_de to `anti_join` this with train_toc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 <- train_toc %>% \n  anti_join(stopwords_de)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"token\"\n```\n:::\n:::\n\n\n## Show The Important Words\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 <- train_toc2 %>% \n  count(token, sort = TRUE) \n```\n:::\n\n\n### Plot\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_toc2 %>% \n  slice_head(n=20) %>%  \n  ggplot()+\n  aes(y=reorder(factor(token), n), x = n, color = token)+\n  geom_col(aes(fill = token, alpha = 2.5)) +\n  ggtitle(\"The most used words\") +\n  ylab(\"token\")+\n  xlab(\"quantity\")+\n  theme_minimal()+\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/unnamed-chunk-12-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n> We see that, to most used word is \"lbr\". We could inspect the dataset way deeper, e. g. do a manual sentimentanalyse or do lemmatization or stem the words. But we will have a look to these types in the different machine learning algorithmen now.\n\n# Preparation\n\n## Define Recipe - rec1 - TF-IDF\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec1 <-\n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_stem(text) %>% \n  step_tfidf(text) %>% \n  step_normalize(all_numeric_predictors())\nrec1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nStemming for text\nTerm frequency-inverse document frequency with text\nCentering and scaling for all_numeric_predictors()\n```\n:::\n:::\n\n\n### Prep & Bake - rec1\n\n::: {.cell}\n\n```{.r .cell-code}\nrec1_prep <- rec1 %>% \n  prep() %>% \n  recipes::bake(new_data = NULL)\n```\n:::\n\n\n## Define Recipe - rec2 - word embedding\n### Insert The Predefined List\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_file_model <- \"C:/Users/sapi-/OneDrive - Hochschule für Angewandte Wissenschaften Ansbach/Desktop/AWM/angewandte Wirtschats- und Medienpsychologie/5. Semester/Word_Embedding/de.300.bin\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.exists(out_file_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfasttext_model <- load_model(out_file_model)\ndictionary <- get_dictionary(fasttext_model)\nget_word_vectors(fasttext_model, c(\"menschen\")) %>% `[`(1:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] -0.043737594 -0.033647023 -0.016398411  0.037433818  0.029863771\n [6] -0.008217440  0.002691153 -0.027484305 -0.058012061  0.004103063\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(head(dictionary, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \",\"    \".\"    \"</s>\" \"und\"  \"der\"  \":\"    \"die\"  \"\\\"\"   \")\"    \"(\"   \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nword_embedding_text <- tibble(word = dictionary)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nwords_vecs <- get_word_vectors(fasttext_model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nword_embedding_text <-\n  word_embedding_text %>% \n  bind_cols(words_vecs)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(word_embedding_text) <- c(\"word\", paste0(\"v\", sprintf(\"%03d\", 1:301)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `value` argument of `names<-` must have the same length as `x` as of tibble\n3.0.0.\ni `names` must have length 301, not 302.\n```\n:::\n:::\n\n\n### Recipe Definition rec2\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2 <- \n  recipe(c1 ~ ., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\n  step_word_embeddings(text, embeddings = word_embedding_text)\n \nrec2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nWord embeddings aggregated from text\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2_prep <- rec2 %>% \n  prep() %>% \n  recipes::bake(new_data = NULL)\n```\n:::\n\n\n## Define Recipe - rec3 - Word Embeddings\n### Define Helperfunctions\nWe are using the package [pradadata] (https://github.com/sebastiansauer/pradadata) from @sebastian_sauer_2018_1996614. The data are licensed according to General Public License 3 (GLP-3).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"schimpwoerter\", package = \"pradadata\")\ndata(\"sentiws\", package = \"pradadata\")\ndata(\"wild_emojis\", package = \"pradadata\")\nsource(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/helper/helper_funs.R\")\n```\n:::\n\n\n## rec3\n\n::: {.cell}\n\n```{.r .cell-code}\nrec3 <-\n  recipe(c1 ~., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_text_normalization(text) %>% \n  step_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>% \n  step_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>% \n  step_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>% \n  step_mutate(text_copy = text) %>% \n  step_textfeature(text_copy) %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_stem(text) %>% \n  step_word_embeddings(text, embeddings = word_embedding_text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrec3_prep <- rec3 %>% \n  prep() %>% \n  recipes::bake(new_data = NULL)\n```\n:::\n\n\n## Define Recipe - rec4 - TF-IDF\n## rec4\n\n::: {.cell}\n\n```{.r .cell-code}\nrec4 <-\n  recipe(c1 ~., data = select(d_train, text, c1, id)) %>% \n  update_role(id, new_role = \"id\") %>% \n  step_text_normalization(text) %>% \n  step_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>% \n  step_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>% \n  step_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>% \n  step_mutate(text_copy = text) %>% \n  step_textfeature(text_copy) %>% \n  step_tokenize(text) %>% \n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>% \n  step_stem(text) %>% \n  step_tfidf(text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrec4_prep <- rec4 %>% \n  prep() %>% \n  recipes::bake(new_data = NULL)\n```\n:::\n\n\n# Build Resamples\nBecause of the large amount of the data and the extreme extensive recipes, we only use the V-Fold-Cross-Validation. \n\n::: {.cell}\n\n```{.r .cell-code}\nfolds <- vfold_cv(data = d_train,\n                  v = 2,\n                  repeats = 1,\n                  strata = c1)\n```\n:::\n\n\n# Build the Penalty-Grid\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda_grid <- grid_regular(penalty(), \n                            levels = 30)\n```\n:::\n\n\n# Build the Models\n## Null model\n\n::: {.cell}\n\n```{.r .cell-code}\nmod0 <- null_model() %>% \n  set_engine(\"parsnip\") %>% \n  set_mode(\"classification\")\n```\n:::\n\n\n## Define The Workflow\n\n::: {.cell}\n\n```{.r .cell-code}\nwf0 <- workflow() %>% \n  add_recipe(rec1) %>% \n  add_model(mod0)\n```\n:::\n\n\n## Resampling & Model Quality\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\ntic()\nfit0 <- fit_resamples(\n  wf0,\n  folds,\n  control =control_resamples(save_pred = TRUE)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'stopwords' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n99.57 sec elapsed\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance0 <- collect_metrics(fit0)\nperformance0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n   std_err .config             \n  <chr>    <chr>      <dbl> <int>     <dbl> <chr>               \n1 accuracy binary     0.663     2 0.0000673 Preprocessor1_Model1\n2 roc_auc  binary     0.5       2 0         Preprocessor1_Model1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npreds0 <- collect_predictions(fit0)\n\npreds0 %>% \n  group_by(id) %>% \n  roc_curve(truth = c1, .pred_OFFENSE) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/unnamed-chunk-36-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat_resampled(fit0, tidy = FALSE) %>% \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/unnamed-chunk-37-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n## Lasso-L1 with TF-IDF\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_8.2_mod <- logistic_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\") %>% \n  set_mode(\"classification\")\nl1_8.2_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define the Workflow\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_8.2_wf <-workflow() %>% \n  add_recipe(rec1) %>% \n  add_model(l1_8.2_mod)\nl1_8.2_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n5 Recipe Steps\n\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality \n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nl1_8.2_wf_fit <- tune_grid(\n  l1_8.2_wf,\n  folds,\n  grid = lambda_grid,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'glmnet' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'Matrix' wurde unter R Version 4.1.3 erstellt\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_8.2_wf_fit_performance <- collect_metrics(l1_8.2_wf_fit)\nl1_8.2_wf_fit_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.725     2 0.00394 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.730     2 0.00210 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.725     2 0.00394 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.730     2 0.00210 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.725     2 0.00394 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.730     2 0.00210 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.725     2 0.00394 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.730     2 0.00210 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.725     2 0.00394 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.730     2 0.00210 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(l1_8.2_wf_fit)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n  penalty .config              \n    <dbl> <chr>                \n1 0.00853 Preprocessor1_Model24\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l1_8.2_wf_fit <- \n  l1_8.2_wf_fit %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l1_8.2_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1 0.00853 roc_auc binary     0.739     2 0.00405 Preprocessor1_Mod~ 0.739  0.735\n```\n:::\n:::\n\n\n\n## Ridge-Regression-L2 with TF-IDF\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_8.3_mod <- logistic_reg(penalty = tune(), mixture = 0) %>% \n  set_engine(\"glmnet\") %>% \n  set_mode(\"classification\")\nl2_8.3_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define the Workflow\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_8.3_wf <-workflow() %>% \n  add_recipe(rec1) %>% \n  add_model(l2_8.3_mod)\nl2_8.3_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n5 Recipe Steps\n\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality \n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nl2_8.3_wf_fit <- tune_grid(\n  l2_8.3_wf,\n  folds,\n  grid = lambda_grid,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(l2_8.3_wf_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.705     2 0.00605 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.750     2 0.00510 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.705     2 0.00605 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.750     2 0.00510 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.705     2 0.00605 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.750     2 0.00510 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.705     2 0.00605 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.750     2 0.00510 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.705     2 0.00605 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.750     2 0.00510 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l2_8.3_wf_fit <- \n  l2_8.3_wf_fit %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l2_8.3_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1       1 roc_auc binary     0.751     2 0.00502 Preprocessor1_Mod~ 0.751  0.746\n```\n:::\n:::\n\n\n## Lasso-L1 with Word Embeddings\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_8.4_mod <- logistic_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\") %>% \n  set_mode(\"classification\")\nl1_8.4_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define the Workflow\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_8.4_wf <- workflow() %>% \n  add_recipe(rec2) %>% \n  add_model(l1_8.4_mod)\nl1_8.4_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n3 Recipe Steps\n\n* step_tokenize()\n* step_stopwords()\n* step_word_embeddings()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality \n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nl1_8.4_wf_fit <- tune_grid(\n  l1_8.4_wf,\n  folds,\n  grid = lambda_grid,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(l1_8.4_wf_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n  std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.701     2 0.000259 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.710     2 0.00796  Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.701     2 0.000259 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.710     2 0.00796  Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.701     2 0.000259 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.710     2 0.00796  Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.701     2 0.000259 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.710     2 0.00796  Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.701     2 0.000259 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.710     2 0.00796  Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l1_8.4_wf_fit <- \n  l1_8.4_wf_fit %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l1_8.4_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1 0.00386 roc_auc binary     0.717     2 0.00593 Preprocessor1_Mod~ 0.718  0.711\n```\n:::\n:::\n\n\n\n## Ridge-Regression-L2 with Word Embeddings\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_8.5_mod <- logistic_reg(penalty = tune(), mixture = 0) %>% \n  set_engine(\"glmnet\") %>% \n  set_mode(\"classification\")\nl2_8.5_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define the Workflow\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_8.5_wf <- workflow() %>% \n  add_recipe(rec2) %>% \n  add_model(l2_8.5_mod)\nl2_8.5_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n3 Recipe Steps\n\n* step_tokenize()\n* step_stopwords()\n* step_word_embeddings()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality \n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nl2_8.5_wf_fit <- tune_grid(\n  l2_8.5_wf,\n  folds,\n  grid = lambda_grid,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(l2_8.5_wf_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.702     2 0.00553 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.717     2 0.00802 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.702     2 0.00553 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.717     2 0.00802 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.702     2 0.00553 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.717     2 0.00802 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.702     2 0.00553 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.717     2 0.00802 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.702     2 0.00553 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.717     2 0.00802 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l2_8.5_wf_fit <- \n  l2_8.5_wf_fit %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l2_8.5_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1   0.452 roc_auc binary     0.715     2  0.0171 Preprocessor1_Mod~ 0.724  0.714\n```\n:::\n:::\n\n\n## Lasso-L1 with Word Embeddings\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_8.6_mod <- logistic_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\") %>% \n  set_mode(\"classification\")\nl1_8.6_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define the Workflow\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_8.6_wf <- workflow() %>% \n  add_recipe(rec3) %>% \n  add_model(l1_8.6_mod)\nl1_8.6_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_word_embeddings()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality \n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nl1_8.6_wf_fit <- tune_grid(\n  l1_8.6_wf,\n  folds,\n  grid = lambda_grid,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'stringi' wurde unter R Version 4.1.2 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'textfeatures' wurde unter R Version 4.1.3 erstellt\n```\n:::\n:::\n\n\n### Performance\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(l1_8.6_wf_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.721     2 0.00254 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.739     2 0.00289 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.721     2 0.00254 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.739     2 0.00289 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.721     2 0.00254 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.739     2 0.00289 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.721     2 0.00254 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.739     2 0.00289 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.721     2 0.00254 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.739     2 0.00289 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n\n### Best Model\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l1_8.6_wf_fit <- \n  l1_8.6_wf_fit %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l1_8.6_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1 0.00853 roc_auc binary     0.754     2 0.00167 Preprocessor1_Mod~ 0.755  0.752\n```\n:::\n:::\n\n\n## Ridge-Regression-L2 with TF-IDF\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_8.7_mod <- logistic_reg(penalty = tune(), mixture = 0) %>% \n  set_engine(\"glmnet\") %>% \n  set_mode(\"classification\")\nl2_8.7_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define the Workflow\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_8.7_wf <-workflow() %>% \n  add_recipe(rec3) %>% \n  add_model(l2_8.7_mod)\nl2_8.7_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_word_embeddings()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality \n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nl2_8.7_wf_fit <- tune_grid(\n  l2_8.7_wf,\n  folds,\n  grid = lambda_grid,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_8.7_wf_performance <- collect_metrics(l2_8.7_wf_fit)\nl2_8.7_wf_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.724     2 0.00334 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.753     2 0.00402 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.724     2 0.00334 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.753     2 0.00402 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.724     2 0.00334 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.753     2 0.00402 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.724     2 0.00334 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.753     2 0.00402 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.724     2 0.00334 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.753     2 0.00402 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(l2_8.7_wf_fit)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n  penalty .config              \n    <dbl> <chr>                \n1  0.0924 Preprocessor1_Model27\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l2_8.7_wf_fit <- \n  l2_8.7_wf_fit %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l2_8.7_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1   0.204 roc_auc binary     0.757     2  0.0105 Preprocessor1_Mod~ 0.759  0.751\n```\n:::\n:::\n\n\n\n## Lasso-L1 with TF-IDF\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_8.8_mod <- logistic_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\") %>% \n  set_mode(\"classification\")\nl1_8.8_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define the Workflow\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_8.8_wf <-workflow() %>% \n  add_recipe(rec4) %>% \n  add_model(l1_8.8_mod)\nl1_8.8_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality \n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nl1_8.8_wf_fit <- tune_grid(\n  l1_8.8_wf,\n  folds,\n  grid = lambda_grid,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl1_8.8_wf_performance <- collect_metrics(l1_8.8_wf_fit)\nl1_8.8_wf_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.736     2 0.00154 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.761     2 0.00775 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.736     2 0.00154 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.761     2 0.00775 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.736     2 0.00154 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.761     2 0.00775 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.736     2 0.00154 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.761     2 0.00775 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.736     2 0.00154 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.761     2 0.00775 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(l1_8.8_wf_fit)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n  penalty .config              \n    <dbl> <chr>                \n1 0.00853 Preprocessor1_Model24\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l1_8.8_wf_fit <- \n  l1_8.8_wf_fit %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l1_8.8_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1 0.00853 roc_auc binary     0.773     2 0.00350 Preprocessor1_Mod~ 0.773  0.769\n```\n:::\n:::\n\n\n## Ridge-Regression-L2 with TF-IDF\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_8.9_mod <- logistic_reg(penalty = tune(), mixture = 0) %>% \n  set_engine(\"glmnet\") %>% \n  set_mode(\"classification\")\nl2_8.9_mod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Define the Workflow\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_8.9_wf <-workflow() %>% \n  add_recipe(rec4) %>% \n  add_model(l2_8.9_mod)\nl2_8.9_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### Resampling & Model Quality \n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\nl2_8.9_wf_fit <- tune_grid(\n  l2_8.9_wf,\n  folds,\n  grid = lambda_grid,\n  control = control_resamples(save_pred = TRUE)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nl2_8.9_wf_performance <- collect_metrics(l2_8.9_wf_fit)\nl2_8.9_wf_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.711     2 0.00385 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.763     2 0.00446 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.711     2 0.00385 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.763     2 0.00446 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.711     2 0.00385 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.763     2 0.00446 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.711     2 0.00385 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.763     2 0.00446 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.711     2 0.00385 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.763     2 0.00446 Preprocessor1_Model05\n# ... with 50 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(l2_8.9_wf_fit)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n       penalty .config              \n         <dbl> <chr>                \n1 0.0000000001 Preprocessor1_Model01\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nchosen_auc_l2_8.9_wf_fit <- \n  l2_8.9_wf_fit %>%\n  select_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l2_8.9_wf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1       1 roc_auc binary     0.763     2 0.00446 Preprocessor1_Mod~ 0.763  0.758\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}