[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Sabrina",
    "section": "",
    "text": "Hello, my name is Sabrina Michl and this is my new blog about Data Science.\nCurrently I am studying business and media psychology and my major courses are Data Science (suprise) and New Work.\nWith this blog I want to give you insights from the fascinating world of data science.\nI‚Äôm happy to answer any questions you may have and I‚Äôm happy to receive links on LinkedIn.\nHave fun & do not hesitate if you have any question.\nKind regards, Sabrina üòä"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Text Mining for AWM",
    "section": "",
    "text": "Text Mining\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nSabrina Michl\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2023\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\n\n3 + 3\n\n[1] 6"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Text-Mining/index.html",
    "href": "posts/Text-Mining/index.html",
    "title": "Text Mining",
    "section": "",
    "text": "For this analysis we use the dataset from Wiegand (2019a) out of the zip archive Wiegand (2019b). The data are licensed according to Attribution 4.0 International (CC-BY-4.0). The used wordembeddings are from Grave et al. (2018). The data are licensed according to Attribution-ShareAlike 3.0 Unported (CC-BY-SA 3.0). The picture, that is used is from Altmann (n.d.)."
  },
  {
    "objectID": "posts/Text-Mining/index.html#trainingsdatensatz",
    "href": "posts/Text-Mining/index.html#trainingsdatensatz",
    "title": "Datenauswertung",
    "section": "Trainingsdatensatz",
    "text": "Trainingsdatensatz\n\nd_train <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.training.txt\", col_names = FALSE)\n\nRows: 5009 Columns: 3\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr (3): X1, X2, X3\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nSpalten umbenennen\n\nnames(d_train) <- c(\"text\", \"c1\", \"c2\")\n\n\n\nID-Spalte einf√ºgen\n\nd_train <- d_train %>% \n  mutate(id = row_number()) %>% \n  select(id, everything())"
  },
  {
    "objectID": "posts/Text-Mining/index.html#testdatensatz",
    "href": "posts/Text-Mining/index.html#testdatensatz",
    "title": "Datenauswertung",
    "section": "Testdatensatz",
    "text": "Testdatensatz\n\nd_test <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.test.txt\", col_names = FALSE)\n\n\nSpalten umbenennen\n\nnames(d_test) <- c(\"text\", \"c1\", \"c2\")\n\n\n\nID-Spalte einf√ºgen\n\nd_test <- d_test %>% \n  mutate(id = row_number()) %>% \n  select(id, everything())"
  },
  {
    "objectID": "posts/Text-Mining/index.html#train-dataset",
    "href": "posts/Text-Mining/index.html#train-dataset",
    "title": "Text Mining",
    "section": "3.1 Train Dataset",
    "text": "3.1 Train Dataset\n\nd_train <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.training.txt\", col_names = FALSE)\n\n\nRename Columns\n\nnames(d_train) <- c(\"text\", \"c1\", \"c2\")\n\n\n\nAdd ID Column\n\nd_train <- d_train %>%\nmutate(id = row_number()) %>%\nselect(id, everything())"
  },
  {
    "objectID": "posts/Text-Mining/index.html#test-dataset",
    "href": "posts/Text-Mining/index.html#test-dataset",
    "title": "Text Mining",
    "section": "3.2 Test Dataset",
    "text": "3.2 Test Dataset\n\nd_test <- read_tsv(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/daten/germeval2018.test.txt\", col_names = FALSE)\n\n\nRename Columns\n\nnames(d_test) <- c(\"text\", \"c1\", \"c2\")\n\n\n\nAdd ID Column\n\nd_test <- d_test %>%\nmutate(id = row_number()) %>%\nselect(id, everything())"
  },
  {
    "objectID": "posts/Text-Mining/index.html#insert-stopwords_de",
    "href": "posts/Text-Mining/index.html#insert-stopwords_de",
    "title": "Text Mining",
    "section": "Insert stopwords_de",
    "text": "Insert stopwords_de\n\ndata(stopwords_de, package = \"lsa\")\nstopwords_de <- tibble(word = stopwords_de)\nstopwords_de <- stopwords_de %>% \n  rename(token = word)\n\n\nAfter that we use the stopwords_de to anti_join this with train_toc.\n\n\ntrain_toc2 <- train_toc %>% \n  anti_join(stopwords_de)\n\nJoining, by = \"token\""
  },
  {
    "objectID": "posts/Text-Mining/index.html#show-the-important-words",
    "href": "posts/Text-Mining/index.html#show-the-important-words",
    "title": "Text Mining",
    "section": "Show The Important Words",
    "text": "Show The Important Words\n\ntrain_toc2 <- train_toc2 %>%\ncount(token, sort = TRUE)\n\n\nPlot\n\ntrain_toc2 %>%\nslice_head(n=20) %>%\nggplot()+\naes(y=reorder(factor(token), n), x = n, color = token)+\ngeom_col(aes(fill = token, alpha = 2.5)) +\nggtitle(\"The most used words\") +\nylab(\"token\")+\nxlab(\"quantity\")+\ntheme_minimal()+\ntheme(legend.position = \"none\")\n\n\n\n\n\nWe see that, to most used word is ‚Äúlbr‚Äù. We could inspect the dataset way deeper, e. g. do a manual sentimentanalyse or do lemmatization or stem the words. But we will have a look to these types in the different machine learning algorithmen now. # Preparation ## Define Recipe - rec1 - TF-IDF"
  },
  {
    "objectID": "posts/Text-Mining/index.html#define-recipe---rec1---tf-idf",
    "href": "posts/Text-Mining/index.html#define-recipe---rec1---tf-idf",
    "title": "Text Mining",
    "section": "5.1 Define Recipe - rec1 - TF-IDF",
    "text": "5.1 Define Recipe - rec1 - TF-IDF\n\nrec1 <-\nrecipe(c1 ~ ., data = select(d_train, text, c1, id)) %>%\nupdate_role(id, new_role = \"id\") %>%\nstep_tokenize(text) %>%\nstep_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\nstep_stem(text) %>%\nstep_tfidf(text) %>%\nstep_normalize(all_numeric_predictors())\nrec1\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nStemming for text\nTerm frequency-inverse document frequency with text\nCentering and scaling for all_numeric_predictors()\n\n\n\nPrep & Bake - rec1\n\nrec1_prep <- rec1 %>%\nprep() %>%\nrecipes::bake(new_data = NULL)"
  },
  {
    "objectID": "posts/Text-Mining/index.html#define-recipe---rec2---word-embedding",
    "href": "posts/Text-Mining/index.html#define-recipe---rec2---word-embedding",
    "title": "Text Mining",
    "section": "5.2 Define Recipe - rec2 - word embedding",
    "text": "5.2 Define Recipe - rec2 - word embedding\n\nInsert The Predefined List\n\nout_file_model <- \"C:/Users/sapi-/OneDrive - Hochschule f√ºr Angewandte Wissenschaften Ansbach/Desktop/AWM/angewandte Wirtschats- und Medienpsychologie/5. Semester/Word_Embedding/de.300.bin\"\n\n\nfile.exists(out_file_model)\n\n[1] TRUE\n\n\n\nfasttext_model <- load_model(out_file_model)\ndictionary <- get_dictionary(fasttext_model)\nget_word_vectors(fasttext_model, c(\"menschen\")) %>% `[`(1:10)\n\n [1] -0.043737594 -0.033647023 -0.016398411  0.037433818  0.029863771\n [6] -0.008217440  0.002691153 -0.027484305 -0.058012061  0.004103063\n\n\n\nprint(head(dictionary, 10))\n\n [1] \",\"    \".\"    \"</s>\" \"und\"  \"der\"  \":\"    \"die\"  \"\\\"\"   \")\"    \"(\"   \n\n\n\nword_embedding_text <- tibble(word = dictionary)\n\n\noptions(mc.cores = parallel::detectCores())\nwords_vecs <- get_word_vectors(fasttext_model)\n\n\nword_embedding_text <-\nword_embedding_text %>%\nbind_cols(words_vecs)\n\n\nnames(word_embedding_text) <- c(\"word\", paste0(\"v\", sprintf(\"%03d\", 1:301)))\n\nWarning: The `value` argument of `names<-` must have the same length as `x` as of tibble\n3.0.0.\ni `names` must have length 301, not 302.\n\n\n\n\nRecipe Definition rec2\n\ntic()\n\n\nrec2 <-\nrecipe(c1 ~ ., data = select(d_train, text, c1, id)) %>%\nupdate_role(id, new_role = \"id\") %>%\nstep_tokenize(text) %>%\nstep_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\nstep_word_embeddings(text, embeddings = word_embedding_text)\nrec2\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          1\n predictor          1\n\nOperations:\n\nTokenization for text\nStop word removal for text\nWord embeddings aggregated from text\n\n\n\nrec2_prep <- rec2 %>%\nprep() %>%\nrecipes::bake(new_data = NULL)"
  },
  {
    "objectID": "posts/Text-Mining/index.html#define-recipe---rec3",
    "href": "posts/Text-Mining/index.html#define-recipe---rec3",
    "title": "Text Mining",
    "section": "5.3 Define recipe - rec3",
    "text": "5.3 Define recipe - rec3\n\nDefine helperfunctions\nNow we are using the predefined list of Dirty, Naughty, Obscene and Otherwise Bad Words Patch (n.d.)\n\nschimpf1 <- import(\"https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/de\", format = \",\", header = FALSE)"
  },
  {
    "objectID": "posts/Text-Mining/index.html#define-recipe---rec3---word-embeddings",
    "href": "posts/Text-Mining/index.html#define-recipe---rec3---word-embeddings",
    "title": "Text Mining",
    "section": "5.3 Define Recipe - rec3 - Word Embeddings",
    "text": "5.3 Define Recipe - rec3 - Word Embeddings\n\nInsert the Helperfunctions\nWe are using the package [pradadata] (https://github.com/sebastiansauer/pradadata) from Sauer (2018). The data are licensed according to General Public License 3 (GLP-3).\n\ndata(\"schimpwoerter\", package = \"pradadata\")\ndata(\"sentiws\", package = \"pradadata\")\ndata(\"wild_emojis\", package = \"pradadata\")\nsource(\"C:/Users/sapi-/OneDrive/Studium/5. Semester/Data Science II/Data_Science_Blog/helper/helper_funs.R\")\n\n\n\nrec3\n\nrec3 <-\nrecipe(c1 ~., data = select(d_train, text, c1, id)) %>%\nupdate_role(id, new_role = \"id\") %>%\nstep_text_normalization(text) %>%\nstep_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>%\nstep_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>%\nstep_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>%\nstep_mutate(text_copy = text) %>%\nstep_textfeature(text_copy) %>%\nstep_tokenize(text) %>%\nstep_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\nstep_stem(text) %>%\nstep_word_embeddings(text, embeddings = word_embedding_text)\n\n\nrec3_prep <- rec3 %>%\nprep() %>%\nrecipes::bake(new_data = NULL)"
  },
  {
    "objectID": "posts/Text-Mining/index.html#define-recipe---rec4---tf-idf",
    "href": "posts/Text-Mining/index.html#define-recipe---rec4---tf-idf",
    "title": "Text Mining",
    "section": "5.4 Define Recipe - rec4 - TF-IDF",
    "text": "5.4 Define Recipe - rec4 - TF-IDF\n\nrec4\n\nrec4 <-\nrecipe(c1 ~., data = select(d_train, text, c1, id)) %>%\nupdate_role(id, new_role = \"id\") %>%\nstep_text_normalization(text) %>%\nstep_mutate(emo_count = map_int(text, ~count_lexicon(.x, sentiws$word))) %>%\nstep_mutate(schimpf_count = map_int(text, ~count_lexicon(.x, schimpfwoerter$word))) %>%\nstep_mutate(wild_emojis = map_int(text, ~count_lexicon(.x, wild_emojis$emoji))) %>%\nstep_mutate(text_copy = text) %>%\nstep_textfeature(text_copy) %>%\nstep_tokenize(text) %>%\nstep_stopwords(text, language = \"de\", stopword_source = \"snowball\") %>%\nstep_stem(text) %>%\nstep_tfidf(text)\n\n\nrec4_prep <- rec4 %>%\nprep() %>%\nrecipes::bake(new_data = NULL)"
  },
  {
    "objectID": "posts/Text-Mining/index.html#null-model",
    "href": "posts/Text-Mining/index.html#null-model",
    "title": "Text Mining",
    "section": "8.1 Null Model",
    "text": "8.1 Null Model\n\nmod0 <- null_model() %>%\nset_engine(\"parsnip\") %>%\nset_mode(\"classification\")\n\n\nDefine The Workflow\n\nwf0 <- workflow() %>%\nadd_recipe(rec1) %>%\nadd_model(mod0)\n\n\n\nResampling & Model Quality\n\noptions(mc.cores = parallel::detectCores())\ntic()\nfit0 <- fit_resamples(\nwf0,\nfolds,\ncontrol =control_resamples(save_pred = TRUE)\n)\n\nWarning: Paket 'stopwords' wurde unter R Version 4.1.3 erstellt\n\ntoc()\n\n95.39 sec elapsed\n\n\n\nperformance0 <- collect_metrics(fit0)\nperformance0\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n   std_err .config             \n  <chr>    <chr>      <dbl> <int>     <dbl> <chr>               \n1 accuracy binary     0.663     2 0.0000673 Preprocessor1_Model1\n2 roc_auc  binary     0.5       2 0         Preprocessor1_Model1\n\n\n\npreds0 <- collect_predictions(fit0)\npreds0 %>%\ngroup_by(id) %>%\nroc_curve(truth = c1, .pred_OFFENSE) %>%\nautoplot()\n\n\n\n\n\nconf_mat_resampled(fit0, tidy = FALSE) %>%\nautoplot(type = \"heatmap\")"
  },
  {
    "objectID": "posts/Text-Mining/index.html#lasso-l1-with-tf-idf",
    "href": "posts/Text-Mining/index.html#lasso-l1-with-tf-idf",
    "title": "Text Mining",
    "section": "8.2 Lasso-L1 With TF-IDF",
    "text": "8.2 Lasso-L1 With TF-IDF\n\nL1-Model\n\nl1_8.2_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl1_8.2_mod\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nDefine The Workflow\n\nl1_8.2_wf <-workflow() %>%\nadd_recipe(rec1) %>%\nadd_model(l1_8.2_mod)\nl1_8.2_wf\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n5 Recipe Steps\n\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nResampling & Model Quality\n\noptions(mc.cores = parallel::detectCores())\nl1_8.2_wf_fit <- tune_grid(\nl1_8.2_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n\nWarning: Paket 'glmnet' wurde unter R Version 4.1.3 erstellt\n\n\nWarning: Paket 'Matrix' wurde unter R Version 4.1.3 erstellt\n\n\n\nl1_8.2_wf_fit_performance <- collect_metrics(l1_8.2_wf_fit)\nl1_8.2_wf_fit_performance\n\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n  std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.720     2 0.00733  Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.729     2 0.000785 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.720     2 0.00733  Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.729     2 0.000785 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.720     2 0.00733  Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.729     2 0.000785 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.720     2 0.00733  Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.729     2 0.000785 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.720     2 0.00733  Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.729     2 0.000785 Preprocessor1_Model05\n# ... with 50 more rows\n\n\n\n\nShow The Best\n\nselect_best(l1_8.2_wf_fit)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n# A tibble: 1 x 2\n  penalty .config              \n    <dbl> <chr>                \n1 0.00853 Preprocessor1_Model24\n\n\n\n\nSelect The Best\n\nchosen_auc_l1_8.2_wf_fit <-\nl1_8.2_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l1_8.2_wf_fit\n\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1 0.00853 roc_auc binary     0.737     2 0.00289 Preprocessor1_Mod~ 0.737  0.734"
  },
  {
    "objectID": "posts/Text-Mining/index.html#ridge-regression-l2-with-tf-idf",
    "href": "posts/Text-Mining/index.html#ridge-regression-l2-with-tf-idf",
    "title": "Text Mining",
    "section": "8.3 Ridge-Regression-L2 With TF-IDF",
    "text": "8.3 Ridge-Regression-L2 With TF-IDF\n\nL2-Model\n\nl2_8.3_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl2_8.3_mod\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n\n\n\n\nDefine The Workflow\n\nl2_8.3_wf <-workflow() %>%\nadd_recipe(rec1) %>%\nadd_model(l2_8.3_mod)\nl2_8.3_wf\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n5 Recipe Steps\n\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n\n\n\n\nResampling & Model Quality\n\noptions(mc.cores = parallel::detectCores())\nl2_8.3_wf_fit <- tune_grid(\nl2_8.3_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n\n\ncollect_metrics(l2_8.3_wf_fit)\n\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n  std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.709     2 0.00365  Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.746     2 0.000942 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.709     2 0.00365  Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.746     2 0.000942 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.709     2 0.00365  Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.746     2 0.000942 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.709     2 0.00365  Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.746     2 0.000942 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.709     2 0.00365  Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.746     2 0.000942 Preprocessor1_Model05\n# ... with 50 more rows\n\n\n\n\nSelect The Best\n\nchosen_auc_l2_8.3_wf_fit <-\nl2_8.3_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l2_8.3_wf_fit\n\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1       1 roc_auc binary     0.747     2 0.00112 Preprocessor1_Mod~ 0.747  0.746"
  },
  {
    "objectID": "posts/Text-Mining/index.html#lasso-l1-with-word-embeddings",
    "href": "posts/Text-Mining/index.html#lasso-l1-with-word-embeddings",
    "title": "Text Mining",
    "section": "8.4 Lasso-L1 With Word Embeddings",
    "text": "8.4 Lasso-L1 With Word Embeddings\n\nL1-Model\n\nl1_8.4_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl1_8.4_mod\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nDefine the Workflow\n\nl1_8.4_wf <- workflow() %>%\nadd_recipe(rec2) %>%\nadd_model(l1_8.4_mod)\nl1_8.4_wf\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n3 Recipe Steps\n\n* step_tokenize()\n* step_stopwords()\n* step_word_embeddings()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nResampling & Model Quality\n\noptions(mc.cores = parallel::detectCores())\nl1_8.4_wf_fit <- tune_grid(\nl1_8.4_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n\n\ncollect_metrics(l1_8.4_wf_fit)\n\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.699     2 0.00493 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.697     2 0.00187 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.699     2 0.00493 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.697     2 0.00187 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.699     2 0.00493 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.697     2 0.00187 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.699     2 0.00493 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.697     2 0.00187 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.699     2 0.00493 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.697     2 0.00187 Preprocessor1_Model05\n# ... with 50 more rows\n\n\n\n\nSelect The Best\n\nchosen_auc_l1_8.4_wf_fit <-\nl1_8.4_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l1_8.4_wf_fit\n\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1 0.00386 roc_auc binary     0.706     2 0.00232 Preprocessor1_Mod~ 0.706  0.704"
  },
  {
    "objectID": "posts/Text-Mining/index.html#ridge-regression-l2-with-word-embeddings",
    "href": "posts/Text-Mining/index.html#ridge-regression-l2-with-word-embeddings",
    "title": "Text Mining",
    "section": "8.5 Ridge-Regression-L2 With Word Embeddings",
    "text": "8.5 Ridge-Regression-L2 With Word Embeddings\n\nL2-Model\n\nl2_8.5_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl2_8.5_mod\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n\n\n\n\nDefine the Workflow\n\nl2_8.5_wf <- workflow() %>%\nadd_recipe(rec2) %>%\nadd_model(l2_8.5_mod)\nl2_8.5_wf\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n3 Recipe Steps\n\n* step_tokenize()\n* step_stopwords()\n* step_word_embeddings()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n\n\n\n\nResampling & Model Quality\n\noptions(mc.cores = parallel::detectCores())\nl2_8.5_wf_fit <- tune_grid(\nl2_8.5_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n\n\ncollect_metrics(l2_8.5_wf_fit)\n\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.702     2 0.00753 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.703     2 0.00244 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.702     2 0.00753 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.703     2 0.00244 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.702     2 0.00753 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.703     2 0.00244 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.702     2 0.00753 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.703     2 0.00244 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.702     2 0.00753 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.703     2 0.00244 Preprocessor1_Model05\n# ... with 50 more rows\n\n\n\n\nSelect The Best\n\nchosen_auc_l2_8.5_wf_fit <-\nl2_8.5_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l2_8.5_wf_fit\n\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n  std_err .config           .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>             <dbl>  <dbl>\n1   0.204 roc_auc binary     0.717     2 0.000538 Preprocessor1_Mo~ 0.717  0.717"
  },
  {
    "objectID": "posts/Text-Mining/index.html#lasso-l1-with-word-embeddings-1",
    "href": "posts/Text-Mining/index.html#lasso-l1-with-word-embeddings-1",
    "title": "Text Mining",
    "section": "8.6 Lasso-L1 With Word Embeddings",
    "text": "8.6 Lasso-L1 With Word Embeddings\n\nL1-Model\n\nl1_8.6_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl1_8.6_mod\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nDefine The Workflow\n\nl1_8.6_wf <- workflow() %>%\nadd_recipe(rec3) %>%\nadd_model(l1_8.6_mod)\nl1_8.6_wf\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_word_embeddings()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nResampling & Model Quality\n\noptions(mc.cores = parallel::detectCores())\nl1_8.6_wf_fit <- tune_grid(\nl1_8.6_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n\nWarning: Paket 'stringi' wurde unter R Version 4.1.2 erstellt\n\n\nWarning: Paket 'textfeatures' wurde unter R Version 4.1.3 erstellt\n\n\n\ncollect_metrics(l1_8.6_wf_fit)\n\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n   std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>     <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.718     2 0.000855  Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.728     2 0.0000880 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.718     2 0.000855  Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.728     2 0.0000880 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.718     2 0.000855  Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.728     2 0.0000880 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.718     2 0.000855  Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.728     2 0.0000880 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.718     2 0.000855  Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.728     2 0.0000880 Preprocessor1_Model05\n# ... with 50 more rows\n\n\n\n\nSelect The Best\n\nchosen_auc_l1_8.6_wf_fit <-\nl1_8.6_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l1_8.6_wf_fit\n\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1 0.00853 roc_auc binary     0.743     2 0.00519 Preprocessor1_Mod~ 0.746  0.743"
  },
  {
    "objectID": "posts/Text-Mining/index.html#ridge-regression-l2-with-tf-idf-1",
    "href": "posts/Text-Mining/index.html#ridge-regression-l2-with-tf-idf-1",
    "title": "Text Mining",
    "section": "8.7 Ridge-Regression-L2 with TF-IDF",
    "text": "8.7 Ridge-Regression-L2 with TF-IDF\n\nL2-Model\n\nl2_8.7_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl2_8.7_mod\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n\n\n\n\nDefine The Workflow\n\nl2_8.7_wf <-workflow() %>%\nadd_recipe(rec3) %>%\nadd_model(l2_8.7_mod)\nl2_8.7_wf\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_word_embeddings()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n\n\n\n\nResampling & Model Quality\n\noptions(mc.cores = parallel::detectCores())\nl2_8.7_wf_fit <- tune_grid(\nl2_8.7_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n\n\nl2_8.7_wf_performance <- collect_metrics(l2_8.7_wf_fit)\nl2_8.7_wf_performance\n\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n  std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.724     2 0.000454 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.739     2 0.000694 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.724     2 0.000454 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.739     2 0.000694 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.724     2 0.000454 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.739     2 0.000694 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.724     2 0.000454 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.739     2 0.000694 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.724     2 0.000454 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.739     2 0.000694 Preprocessor1_Model05\n# ... with 50 more rows\n\n\n\n\nShow The Best\n\nselect_best(l2_8.7_wf_fit)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n# A tibble: 1 x 2\n  penalty .config              \n    <dbl> <chr>                \n1   0.204 Preprocessor1_Model28\n\n\n\n\nSelect The Best\n\nchosen_auc_l2_8.7_wf_fit <-\nl2_8.7_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l2_8.7_wf_fit\n\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1   0.452 roc_auc binary     0.748     2 0.00419 Preprocessor1_Mod~ 0.751  0.747"
  },
  {
    "objectID": "posts/Text-Mining/index.html#lasso-l1-with-tf-idf-1",
    "href": "posts/Text-Mining/index.html#lasso-l1-with-tf-idf-1",
    "title": "Text Mining",
    "section": "8.8 Lasso-L1 With TF-IDF",
    "text": "8.8 Lasso-L1 With TF-IDF\n\nL1-Model\n\nl1_8.8_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl1_8.8_mod\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nDefine The Workflow\n\nl1_8.8_wf <-workflow() %>%\nadd_recipe(rec4) %>%\nadd_model(l1_8.8_mod)\nl1_8.8_wf\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\n\nResampling & Model Quality\n\noptions(mc.cores = parallel::detectCores())\nl1_8.8_wf_fit <- tune_grid(\nl1_8.8_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n\n\nl1_8.8_wf_performance <- collect_metrics(l1_8.8_wf_fit)\nl1_8.8_wf_performance\n\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.733     2 0.00145 Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.754     2 0.00214 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.733     2 0.00145 Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.754     2 0.00214 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.733     2 0.00145 Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.754     2 0.00214 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.733     2 0.00145 Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.754     2 0.00214 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.733     2 0.00145 Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.754     2 0.00214 Preprocessor1_Model05\n# ... with 50 more rows\n\n\n\n\nShow The Best\n\nselect_best(l1_8.8_wf_fit)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n# A tibble: 1 x 2\n  penalty .config              \n    <dbl> <chr>                \n1 0.00853 Preprocessor1_Model24\n\n\n\n\nSelect The Best\n\nchosen_auc_l1_8.8_wf_fit <-\nl1_8.8_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l1_8.8_wf_fit\n\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n std_err .config            .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              <dbl>  <dbl>\n1 0.00853 roc_auc binary     0.767     2 0.00335 Preprocessor1_Mod~ 0.767  0.763"
  },
  {
    "objectID": "posts/Text-Mining/index.html#ridge-regression-l2-with-tf-idf-2",
    "href": "posts/Text-Mining/index.html#ridge-regression-l2-with-tf-idf-2",
    "title": "Text Mining",
    "section": "8.9 Ridge-Regression-L2 With TF-IDF",
    "text": "8.9 Ridge-Regression-L2 With TF-IDF\n\nL2-Model\n\nl2_8.9_mod <- logistic_reg(penalty = tune(), mixture = 0) %>%\nset_engine(\"glmnet\") %>%\nset_mode(\"classification\")\nl2_8.9_mod\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n\n\n\n\nDefine The Workflow\n\nl2_8.9_wf <-workflow() %>%\nadd_recipe(rec4) %>%\nadd_model(l2_8.9_mod)\nl2_8.9_wf\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n10 Recipe Steps\n\n* step_text_normalization()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_mutate()\n* step_textfeature()\n* step_tokenize()\n* step_stopwords()\n* step_stem()\n* step_tfidf()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 0\n\nComputational engine: glmnet \n\n\n\n\nResampling & Model Quality\n\noptions(mc.cores = parallel::detectCores())\nl2_8.9_wf_fit <- tune_grid(\nl2_8.9_wf,\nfolds,\ngrid = lambda_grid,\ncontrol = control_resamples(save_pred = TRUE)\n)\n\n\nl2_8.9_wf_performance <- collect_metrics(l2_8.9_wf_fit)\nl2_8.9_wf_performance\n\n# A tibble: 60 x 7\n    penalty .metric  .estimator  mean     n  std_err .config              \n      <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>                \n 1 1   e-10 accuracy binary     0.708     2 0.00725  Preprocessor1_Model01\n 2 1   e-10 roc_auc  binary     0.760     2 0.000873 Preprocessor1_Model01\n 3 2.21e-10 accuracy binary     0.708     2 0.00725  Preprocessor1_Model02\n 4 2.21e-10 roc_auc  binary     0.760     2 0.000873 Preprocessor1_Model02\n 5 4.89e-10 accuracy binary     0.708     2 0.00725  Preprocessor1_Model03\n 6 4.89e-10 roc_auc  binary     0.760     2 0.000873 Preprocessor1_Model03\n 7 1.08e- 9 accuracy binary     0.708     2 0.00725  Preprocessor1_Model04\n 8 1.08e- 9 roc_auc  binary     0.760     2 0.000873 Preprocessor1_Model04\n 9 2.40e- 9 accuracy binary     0.708     2 0.00725  Preprocessor1_Model05\n10 2.40e- 9 roc_auc  binary     0.760     2 0.000873 Preprocessor1_Model05\n# ... with 50 more rows\n\n\n\n\nSelect The Best\n\nchosen_auc_l2_8.9_wf_fit <-\nl2_8.9_wf_fit %>%\nselect_by_one_std_err(metric = \"roc_auc\", -penalty)\nchosen_auc_l2_8.9_wf_fit\n\n# A tibble: 1 x 9\n  penalty .metric .estimator  mean     n  std_err .config           .best .bound\n    <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>             <dbl>  <dbl>\n1       1 roc_auc binary     0.760     2 0.000873 Preprocessor1_Mo~ 0.760  0.759"
  }
]